\chapter{Background}
\label{chapter:background}
Parton distribution functions (PDFs) bridge the gap between short and long range physics, allowing perturbative Quantum Chromodynamics (QCD) to be applied at the hadronic scale. They embody the incalculable strongly coupled dynamics, and are determined by a comparison of perturbative theory with experiment. Once determined, their form is process-independent and so they can be re-deployed in future calculations. 

This section provides some background to PDFs neccesary for understanding the remainder of this thesis. It is divided in to two main parts, being the necessary physics and the necessary methodology of PDF determination.

To review the physics, we begin by looking at the process of deep inelastic scattering (DIS), and how the na\"ive parton model was developed to explain these experimental observations. Next we look at this in the context of QCD, see how PDFs fit into the picture, and how they evolve with the scale of the physics. Finally we briefly touch on hadron-hadron collisions, which along with DIS constitute the bulk of the processes in modern PDF fits.

To review the methodology we consider the NNPDF fitting strategy, explaining how theory and experiment are used together with neural networks to determine PDFs. We distinguish between the NNPDF3.1 methodology, on which the results in Chapters \ref{chapter:mhous} and \ref{chapter:correlations} are based, and the NNPDF4.0 methodology, on which the results in Chapter \ref{chapter:nuclear} are based. We note that the difference between these methodologies doesn't affect the main lines of argument in these chapters.

\section{Physics background}
\subsection{Deep inelastic scattering}
For a more in-depth analysis, see \cite{pinkbook, hm}. In this section we rely heavily on \cite{nikhefnotes, Hartland:2014nha}.

The notion of bombarding matter to uncover its structure has led to many important discoveries in the last hundred or so years, starting with the Geiger-Marsden experiments from 1908-1913 and the subsequent uncovering of the atomic nucleus \cite{nucleus}. In the decades following the discovery of the neutron in 1932, nuclei were probed at higher energies, leading to them being understood in terms of ``form factors" which parametrised their electric and magnetic distributions. At this stage it was clear that they are not point-like particles and so a series of important experiments were carried out in the 1960s at the Stanford Linear Accelerator (SLAC), involving a high energy beam of charged leptons scattering off a stationary hadronic target. This process is known as deep inelastic scattering. 

In this section we will consider the example of electrons incident on protons, as shown in Fig. \ref{fig:dis}. In the deep inelastic regime, there is a large momentum transfer, $q=k-k'$, mediated by a virtual photon. The proton, $P$, with mass $M$ and initial momentum $p$, fragments into some hadronic state $X$, and the electron starts with energy $E$ and momentum $k$ and ends with energy $E'$ and momentum $k'$. The momentum transfer is large enough that the masses of the proton and electron can be neglected. 

It is customary to define some useful variables for help in the analysis, listed in the table below.
\begin{table}[H]
\centering
\begin{tabular}{l|l|l}
  Variable & Definition & Interpretation   \\
 \hline
  $Q^2$ & $- q^2 = -(k-k')^2$   & momentum transfer    \\
  $\nu$ & $p \cdot q = M(E'-E)$ & energy transfer  \\
  $x$   & $\frac{Q^2}{2\nu}$    & scaling parameter \\
  $y$   & $\frac{q \cdot p}{k \cdot p} = 1 - \frac{E'}{E}$ & inelasticity $\in [0,1]$
\end{tabular}
\end{table}
The interaction is made up of a leptonic current (that of the electron) and a hadronic current (the fragmentation of the proton from $P$ to $X$). This means we can express the squared matrix element, $|\mathcal{M}|^2,$ as
\be
\label{eqn:matrixelement}
|\mathcal{M}|^2 = \mathcal{N} \frac{\alpha^2}{q^4} L_{\mu\nu} W^{\mu\nu},
\ee
where $L_{\mu\nu}$ is the leptonic part, determined from perturbative Quantum Electrodynamics (QED), and $W^{\mu\nu}$ is the hadronic part, containing the incalculable strongly coupled dynamics. $\alpha$ is the QED coupling constant and $\mathcal{N}$ is a normalisation constant which can differ according to convention, hence we keep it undefined here so as to render the analysis clearer.
\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{../diagrams/disdiag.pdf}
\caption{Deep inelastic scattering. \label{fig:dis}}
\end{figure}

From QED, for an unpolarised photon beam in the DIS regime we can use the Feynman rules at tree level to write
\bdm
\label{eqn:ltensor}
L_{\mu\nu} = \sum_{spins} \bar{u}(k')\gamma_\mu u(k) \bar{u}(k) \gamma_\nu u(k')
=  Tr \big(\slashed{k'}\gamma_\mu \slashed{k} \gamma_\nu \big)
= 4 \bigg( k_\mu k'_\nu + k_\nu k'_\mu -g_{\mu\nu} k\cdot k' \bigg)
= 4 \bigg(4 k_\mu k_\nu - 2k_\mu q_\nu - 2 k_\nu q_\mu + g_{\mu \nu} q^2\bigg),
\edm
where in the last line we used the fact that the electron is massless so $
0 = k^{'2} = q^2 + k^2 - 2 q \cdot k \implies q^2 = 2 q \cdot k.$ 

Finding the hadronic tensor is more difficult because we lack knowledge of the hadronic states $P$ and $X$, so our only constraints are that $W^{\mu\nu}$ is Lorentz-invariant and that the electromagnetic current must be conserved, so $q \cdot W =0$. Since we are considering only the electromagnetic interaction, we ignore the possibility for $Z$ boson exchange and therefore also require parity conservation. This allows us to write the general form of the tensor as
\bdm
\label{eqn:htensor}
W^{\mu\nu}(p,q) = - \bigg(g^{\mu\nu} - \frac{q^\mu q^\nu}{q^2}\bigg) W_1(p,q)
+ \bigg(p^\mu - q^\mu \frac{p \cdot q}{q^2}\bigg) \bigg(p^\nu  - q^\nu \frac{p \cdot q}{q^2} \bigg) W_2 (p, q),
\edm
where $W_1$ and $W_2$ are scalar functions which encapsulate the strong dynamics. These scalar functions are often written as:
\begin{equation}
\begin{split}
F_1(x,Q^2) &= W_1(p,q); \\
F_2(x,Q^2) &= \nu W_2(p,q); \\
F_L(x,Q^2) &= F_2(x,Q^2) - 2x F_1(x,Q^2),
\end{split}
\end{equation}
and are known as the ``structure functions". Often the hadronic tensor is parametrised in terms of $F_2$ and $F_L$, the latter of which is the longitudinal structure function and encapsulates the longitudinal component. 

We can now combine Eqns. \ref{eqn:ltensor} and \ref{eqn:htensor} in Eqn. \ref{eqn:matrixelement}, making use of the fact that due to current conservation $q^\mu L_{\mu \nu} = 0$ to help simplify things. This leads us to the result:
\bdm
\label{eqn:disamplitude}
|\mathcal{M}|^2 = 16\ \mathcal{N} \frac{\alpha^2}{q^4} \bigg\{ (-2q^2)W_1(p,q) + \bigg( 4(p \cdot k)^2 - 4 (p \cdot q)(p \cdot k)) \bigg)W_2(p,q) \bigg\}.
\edm

\subsection{The parton model}
Carrying out DIS experiments allows us to measure the structure functions for different values of $x$ and $Q^2$. It transpired that no clear $Q^2$ dependence was observed, and this is known as Bj\"orken scaling \cite{Callan:1973pu}. Because $Q^2$ is the photon's squared momentum, it corresponds to the energy at which the hadron is being probed. The fact that the structure functions are not dependent on this suggests that the interaction is point-like. This led to the formulation of the ``parton model", which described the proton as a composite state made up of point-like particles termed ``partons"\cite{Feynman:1969wa, Feynman:1969ej, Feynman:1973xc}. 

Furthermore, $F_L(x)$ was measured to be 0, known as the Callan-Gross relation~\cite{Callan:1968zza, Callan:1973pu}, which suggests that the point-like particles could not absorb longitudinal photons. This fitted in nicely with the quark models developed shortly before \cite{GellMann:1962xb, GellMann:1964nj, Zweig:1964jf, Dothan:1965aa}, which described hadrons in terms of spin-1/2 quarks; spin-1/2 particles cannot interact with longitudinal photons. This was the first experimental evidence for the existence of quarks.

In the DIS regime, $Q^2$ is large and so the virtual photon probes at the short timescale $1/Q$, meaning that the interaction will be effectively instantaneous when compared with the inner proton dynamics which operate at the QCD scale $1/\lambda_{QCD} \sim $ 1 fm.  In the parton model we make the assumption that the partons have only a small momentum transverse to the proton's, and that they are effectively on shell for the interaction ($k^2 \approx 0$). In addition, we consider the process in the infinite momentum frame of the proton, in which its diameter is Lorentz contracted by $M/|\underline{p}|$ (a small number), so we can assume the photon will only interact with one parton because it will only traverse a narrow cross-section of the proton. The updated picture is shown in Fig. \ref{fig:disparton}.
\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{../diagrams/parton_dis.pdf}
\caption{\label{fig:disparton}DIS in the parton model. One parton with momentum $p$ interacts with the virtual photon, and the other partons ``spectate".}
\end{figure}

We parametrise the momentum of the interacting parton as $\xi p$, $\xi \in [0,1]$. The parton in the final state has negligible mass so its momentum squared is zero:
\be
\begin{split}
(\xi p+q)^2 &= 0  \\
\implies 2 \xi p \cdot q + q^2 &= 0 \\
\implies 2 \xi p \cdot q - Q^2 &= 0  \\
\implies \xi &= \frac{Q^2}{2p \cdot q} \equiv x.
\end{split}
\ee
This allows us to identify the parton's momentum fraction in this frame with the Bj\"orken $x$ variable.

We can think of the total collection of interactions in terms of a weighted sum over the interactions between the photon and the individual point-like partons, integrated over the possible parton momentum fractions. So we can write the proton-level hadronic tensor, $W_{\mu\nu}$ in terms of the parton-level ones, $\hat{W}^q_{\mu\nu}$, as
\be
\begin{split}
W_{\mu\nu} &= \int \d \xi \sum_q  f_q(\xi) \hat{W}^q_{\mu\nu} \delta(Q^2 - 2\xi p \cdot q) \\
&= \int \d \xi \frac{1}{Q^2} \sum_q  f_q(\xi) \hat{W}^q_{\mu\nu} \delta(1 - \frac{2\xi p \cdot q}{Q^2}),
\end{split}
\ee
where $q$ runs over the possible quark flavours and $f_q$ are distributions, with $f_q(\xi)d \xi$ giving the probability that in an interaction a parton of flavour $q$ will be found in the momentum range $\xi \to \xi +d \xi$. We call these functions ``parton distribution functions" (PDFs). The delta function appears due to integration over the final phase space of $X$, and enforces conservation of momentum. Using Eqn \ref{eqn:matrixelement}, we can see that
\be
\begin{split}
\label{eqn:ampcomparison}
|\mathcal{M}|^2 &= \int \d \xi \frac{1}{Q^2} \sum_q  f_q(\xi) |\mathcal{\hat{M}}_q|^2 \delta(1 - \frac{2\xi p \cdot q}{Q^2}) \\
&= \frac{1}{Q^2} \sum_q  f_q(x) |\mathcal{\hat{M}}_q|^2.
\end{split}
\ee
This means that the total amplitude can be expressed in terms of the partonic amplitudes and the PDFs. If we assume that the partons are massless Dirac particles, we can draw a mathematical equivalence with electron-muon scattering. In this scenario the electron has a current like Eqn. \ref{eqn:ltensor}, and the muon has the same, but with the substitutions $k \to p$ and $q \to -q$. Once again we can use $q_\mu L^{\mu \nu} =0$ and the expression
\be
|\mathcal{M}_{(e \mu)}|^2 = \mathcal{N} \frac{\alpha^2}{q^4} L^{(e)}_{\mu\nu} L_{(\mu)}^{\mu\nu}
\ee
to show (in the massless limit)
\be 
|\mathcal{M}_{(e \mu)}|^2 = 16\ \mathcal{N} \frac{\alpha^2}{q^4} \bigg( 16 (p \cdot k)^2 + 8 q^2 (p \cdot k) + 2 q^4 \bigg).
\ee
Using the symmetry of Fig. \ref{fig:disparton}, we can see this is analogous to $|\mathcal{\hat{M}}_q|^2$ under the substitution $p \to xp$, provided we replace the charge of the electron, $e$, with that of the parton, $e_q$, so that $\alpha \to e_q \alpha$. Making use of the expression $p \cdot k = Q^2/2xy$,
\be 
\begin{split}
|\mathcal{\hat{M}}_q|^2 &= 16\ \mathcal{N} \frac{e_q^2 \alpha^2}{q^4} \bigg\{ 4 (2xp \cdot k)^2 + 4 (2 x p\cdot k) q^2 + 2q^4 \bigg\} \\
&= 16\ \mathcal{N} \frac{e_q^2 \alpha^2}{Q^4} \bigg\{ 4 \bigg(\frac{Q^2}{y}\bigg)^2 - 4 \bigg(\frac{Q^2}{y}\bigg) Q^2 + 2Q^4 \bigg\} \\ 
&= 16\ \mathcal{N} e_q^2 \alpha^2 \bigg\{ 2 + 4 \bigg( \frac{1-y}{y^2} \bigg) \bigg\}.
\end{split}
\ee 
Now we can use this alongside Eqn. \ref{eqn:disamplitude} in  Eqn. \ref{eqn:ampcomparison}, giving us
\be 
\begin{split}
F_1 &\equiv W_1 = \sum_q f_q(x)e_q^2, \\
F_2 &\equiv \nu W_2 = 2x \sum_q f_q(x) e_q^2.
\end{split}
\ee
We see immediately that the Callan-Gross relation, $F_L(x) \equiv F_2(x) - 2x F_1(x) = 0$, is satisfied, as was observed experimentally.

However, it was soon observed that this relation was not exact, which is known as ``scaling violation". In order to understand this behaviour it is necessary to revisit the parton model in the light of Quantum Chromodynamics (QCD).

\subsection{Quantum Chromodynamics (QCD)}
QCD is the theory of the strong force. This is responsible for binding together hadrons, and explains the short-range interactions which occur within them. It is a gauge theory where the quark fields are realised as fundamental representations of the $SU(3)$ symmetry group and interactions between them are carried out via gauge bosons termed ``gluons", which are expressed in the adjoint representation \cite{grinstein2006introductory}. 

Quark models showed that the structure of observed hadrons can be explained using the $SU(3)_f$ group alongside the association of quarks with different ``flavours"  \cite{GellMann:1962xb, GellMann:1964nj, Zweig:1964jf, Dothan:1965aa} . The additional $SU(3)_c$ colour symmetry was put forwards in order that the quarks satisfied Fermi-Dirac statistics \cite{Greenberg:1964pe}. Each quark is assigned an additional colour ((anti-)red, green or blue) in such a way that the composite hadrons are colourless. The additional local symmetry is accompanied by eight gauge bosons, the gluons. Colour is the charge of QCD, just as electric charge is for QED. An important difference is that, unlike chargeless photons in QED, the gluons themselves also have colour and this leads to complex self-interactions. 


QCD can be expressed through the Lagrangian
\be
\mathcal{L} = -\frac{1}{4}F_{\mu \nu}^a F^{a \mu \nu} + \bar{q}^i(i \slashed{\mathcal{D}}_i^j - m \delta_i^j) q_j,
\ee
where the covariant derivative is
\be 
\mathcal{D}_\mu \psi(x) = (\partial_\mu -i\sqrt{4 \pi \alpha_s}T^a A_\mu^a) \psi(x) ,
\ee
and the field strength tensor is
\be 
F^a_{\mu \nu} = \partial_\mu A_\nu^a - \partial_\nu A_\mu^a + \sqrt{4 \pi \alpha_s} f^{abc} A_\mu^b A_\nu^c.
\ee
The indices $\mu, \nu$ are spacetime indices, $i, j$ are quark colour indices and $a,b, c$ are gluon colour indices. The first term in the Lagrangian arises from the self-interacting gluons, $A$, and the second term from the quarks, $q$, which obey the Dirac equation. $\alpha_s$ is the strong coupling constant, which dictates the strength of the interaction, and $T^a$ are the eight $SU(3)$ generators. $f^{abc}$ are the SU(3) structure constants. For simplicity we have assumed all quarks have the same mass, $m$. Note that gauge fixing and ghost terms are omitted. For more information see \cite{pinkbook}.

Colour self-interactions give rise to the important properties of ``confinement" and ``asymptotic freedom". The QCD potential is of the form
\be 
V(r) \sim \frac{\alpha}{r} + kr,
\ee
where the first term drops off with distance like QED, but the second term comes from the self-interactions and means that separating two quarks takes infinite energy. This explains why we have not observed free quarks (``confinement"). Additionally, the QCD colour charge decreases with shorter distances. This means that at very short distances or high energies the quarks become ``free", which is known as ``asymptotic freedom". This crucial fact allows us to apply the tool of perturbation theory in such regimes. 

QCD is subject to divergences in the ultra-violet (high energies) and infra-red (low energies). The former are regulated by renormalisation, which introduces a ``renormalisation scale", $\mu_R$. This is non-physical, and so observables cannot depend on it. This observation leads to a ``renormalisation group equation" (RGE), which can be solved by the introduction of a running coupling, dependent on the scale $Q^2$ (i.e. $\alpha_s \to \alpha_s(Q^2)$), which satisfies
\be
\label{eqn:rge}
Q^2 \frac{\partial \alpha_s}{\partial Q^2} = \beta (\alpha_s),
\ee
The beta function, $\beta(\alpha_s)$, can be expressed perturbatively as an expansion in $\alpha_s$ and is currently known to N$^3$LO.

At one-loop order the solution of this equation is
\be 
\alpha_s(Q^2) = \frac{\alpha_s(\mu_R^2)}{1+ \beta_0 \alpha_s (\mu_R^2) \ln\big(\frac{Q^2}{\mu_R^2}\big)},
\ee
where $\beta_0$ is the first coefficient of the $\beta$ expansion. From this solution, taking into account that $\beta_0$ is positive, we can explicitly see asymptotic freedom because $\alpha_s$ decreases as the energy scale increases. 
We also see the role of the renormalisation scale in specifying a particular reference value for $\alpha_s$. In particular, the scale at which the coupling constant starts to diverge is known as the QCD scale, $\Lambda$, and is of order 100 MeV. 

%This solution is not exact because the RGE (Eqn.~\ref{eqn:rge}) only holds to all orders. Any residual $\mu_R$ dependence characterises the accuracy of our calculation, because going to higher and higher orders should reduce this dependence, eventually to 0.

Quantities are infrared safe if they do not depend on long-distance physics. This means we can apply perturbation theory because $\alpha_s$ is small enough in the short-distance regime. Unforunately, at the partonic level, structure functions and cross sections are not infrared safe. 

\subsection{The QCD improved parton model and factorisation}

In the na\"ive parton model, we did not include any interactions involving gluons; their incorporation leads to the QCD improved parton model. The addition of gluons leads to significant complications, owing to the fact that the interacting quarks are free to emit gluons at some stage before detection (remember the detector is at long-distance so we cannot ignore the long-distance physics). When these gluons are ``soft" (low energy) or collinear to one of the partons we run into infrared divergences. This situation is equivalent to the internal propagator quark going on-shell, or in other words there is a large time separation between the partonic interaction and the gluon emission. The observed violation of Bj\"orken scaling has its origins in interactions with gluons. In infrared-safe observables the soft and collinear divergences exactly cancel \cite{Kinoshita:1962ur, Lee:1964is}, but for other cases we need a way of dealing with the disparate short and long range physics. 

This is done using the factorisation theorem \cite{Collins:1989gx}, which allows us to factorise the incalculable long-distance physics into the PDFs, meaning we are able to use perturbative QCD as a predictive theory. The PDFs are then non-perturbative, meaning we must obtain them from experiments, but they are universal quantities and so once determined can be applied everywhere, much like the coupling constants. This process introduces the artificial ``factorisation scale", $\mu_F$, in addition to the renormalisation scale. The factorisation scale separates the short and long distance physics; loosely, if a parton's transverse momentum is less than $\mu_F$ it is considered part of the hadron and is factored into the PDFs, otherwise it is seen as taking part in the hard scattering process, and will appear in the partonic cross section.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{../diagrams/improvedparton_dis.pdf}
\caption{\label{fig:improvedparton} Factorisation and the QCD improved parton model}
\end{figure}


We can write a DIS cross section as
\be
\label{eqn:disfact}
\sigma^{DIS} = \sum_i \int dx f_i(x, \mu_F^2) \hat{\sigma}_i \bigg(x, \frac{Q^2}{\mu_F^2} \bigg),
\ee
corresponding to Fig \ref{fig:improvedparton}, where $i$ runs over partons.

We can see how this works in practice by considering the case where a quark emits a gluon before interaction with the photon, such as in Fig. \ref{fig:scalingviolation}. Here the parent parton, with fraction $y$ of the proton's momentum, emits a gluon giving rise to a daughter parton with a fraction $z$ of the parent hadron's momentum. We can see that $z = x/y$.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{../diagrams/scalingviolation.pdf}
\caption{\label{fig:scalingviolation} A quark radiating a gluon before interacting.}
\end{figure}

It transpires (see \cite{hm} for the derivation) that the structure function $F_2$ can be expressed as
\begin{dmath}
\label{eqn:stfn}
\frac{F_2(x,Q^2)}{x} = \sum_i e_i^2 \int_x^1 \frac{dy}{y} f_i(y) \bigg[ \delta \bigg( 1- \frac{x}{y} \bigg) + \frac{\alpha_s}{2 \pi} \mathcal{P}_{qq} \bigg( \frac{x}{y} \bigg) \ln \bigg( \frac{Q^2}{m^2} \bigg) \bigg].
\end{dmath}
$m$ is a cutoff introduced to regularise the collinear divergence and you can see that as $m \to 0$ the structure function diverges. A divergence also occurs for $(1-z) \to 0$, and this is a soft divergence because it corresponds to the gluon being emitted with zero momentum. The quantity $\mathcal{P}_{qq}$ is the quark-quark ``splitting function", detailing the probability that a quark emits a gluon leaving a daughter quark with fraction $z$ of the parent's momentum. In the $\overline{MS}$ renormalisation scheme this has the form
\be 
\mathcal{P}_{qq} = \frac{4}{3} \bigg( \frac{1+z^2}{1-z} \bigg).
\ee
We want an expression which is free from the soft and collinear divergences. We can proceed by defining
\be 
\mathcal{I}^i_{qq}(x) \equiv \frac{\alpha_s}{2 \pi} \int_x^1 \frac{dy}{y} f_i(y)\mathcal{P}_{qq} \bigg( \frac{x}{y} \bigg),
\ee
and separating \ref{eqn:stfn} into a singular part and a calculable part, like
\begin{dmath}
\frac{F_2(x,Q^2)}{x} = \sum_i e_i^2 \bigg[ f_i(x) + \mathcal{I}^i_{qq}(x)  \ln \bigg( \frac{\mu_F^2}{m^2} \bigg) + \mathcal{I}^i_{qq}(x)  \ln \bigg( \frac{Q^2}{\mu_F^2} \bigg) \bigg].
\end{dmath}
Notice we introduced the artificial factorisation scale, $\mu_F$, to do this. Grouping the singular terms together as
\be
f_i(x, \mu_F^2) =  f_i(x) + \mathcal{I}^i_{qq}(x)  \ln \bigg( \frac{\mu_F^2}{m^2} \bigg),
\ee
we have factorised the divergences into the PDF $f_i(x)$, giving a new PDF, $f_i(x, \mu_F^2)$ , which also depends on $\mu_F$.
Noting that at leading order $f_i(y) = f_i(y, \mu_F^2)$, we are able to write
\begin{dmath}
\frac{F_2(x,Q^2)}{x} = \sum_i e_i^2 \bigg[ f_i(x, \mu_F^2) + \frac{\alpha_s}{2 \pi} \int_x^1 \frac{dy}{y} f_i(y, \mu_F^2)\mathcal{P}_{qq} \bigg( \frac{x}{y} \bigg) \ln \bigg( \frac{Q^2}{\mu_F^2} \bigg) \bigg]+ \mathcal{O}(\alpha_s^2).
\end{dmath}
We know that $F_2$ is an observable quantity and thus should be independent of $\mu_F$, leading to a RGE:
\be
\begin{split}
\frac{1}{e_i^2x} \frac{\partial F_2(x,Q^2)}{\partial \ln \mu_F^2} &= \frac{\partial f_i(x,\mu_F^2)}{\partial \ln \mu_F^2} \\ &+ \frac{\alpha_s}{2 \pi} \int_x^1 \frac{dy}{y} \bigg( \frac{\partial f_i(y,\mu_F^2)}{\partial \ln \mu_F^2}\ln \bigg( \frac{Q^2}{\mu_F^2} \bigg) -  f_i(y, \mu_F^2) \bigg)\mathcal{P}_{qq} \bigg( \frac{x}{y} \bigg)\\ &= 0.
\end{split}
\ee
This can be further simplified by noting that $\frac{\partial f_i(y,\mu_F^2)}{\partial \ln \mu_F^2}$ is of $\mathcal{O}(\alpha_s^2)$, and so
\be
\frac{\partial f_i(x,\mu_F^2)}{\partial \ln \mu_F^2}  =  \frac{\alpha_s}{2 \pi} \int_x^1 \frac{dy}{y} f_i(y, \mu_F^2)\mathcal{P}_{qq} \bigg( \frac{x}{y} \bigg).
\ee
This equation describes the evolution of the newly defined PDFs with scale, a product of the factorisation of the divergences into them. In practice this equation is solved numerically. 

When we also include the gluon as a parton, we open ourselves up to more splitting possibilities (e.g. gluon $\to$ quark and gluon $\to$ gluon), and this result generalises to a set of coupled differential equations known as the DGLAP equations~\cite{jr:altarelli, jr:dokshitzer,jr:gribov}:
\be 
\label{eqn:DGLAP}
\frac{\partial f_i}{ \partial \ln \mu_F^2} = \sum_i \frac{\alpha_s}{2 \pi} \mathcal{P}_{ij} \otimes f_j,
\ee
where we have used the Mellin convolution, defined
\be 
\mathcal{P} \otimes f \equiv \int_x^1 \frac{dy}{y} \mathcal{P} \bigg( \frac{x}{y} \bigg)f(y, \mu_F^2),
\ee
and the index $i$ runs from $-n_f$ to $n_f$ (where $n_f$ is the number of flavours), with the negative indices referring to the antiquarks, 0 to the gluon and the positive ones to the quarks.

The DGLAP equations are commonly dealt with in Mellin space where the convolution is transformed into a product; the Mellin transform from $x$-space to Mellin space is defined as
\be
M(n) \equiv \int_0^1 dx \ x^{n-1} M(x).
\ee
Considering the Mellin transform of the RHS of the DGLAP equation, and suppressing the parton indices, $i$, for clarity,  we have
\be 
\begin{split}
 &\ \int_0^1 dx\ x^{n-1} \ \int_x^1 \frac{dy}{y}\ \frac{\alpha_s}{2 \pi}\ \mathcal{P} \bigg( \frac{x}{y} \bigg)f(y, \mu_F^2) \nonumber\\
&= \int_0^1 dx\ x^{n-1}  \bigg[ \int_0^1 dy \int_0^1 dz\ \frac{\alpha_s}{2 \pi}\ \mathcal{P}(z) f(y) \delta(x-yz) \bigg] \nonumber\\
&= \frac{\alpha_s}{2 \pi} \int_0^1 dz\ z^{n-1}\ \mathcal{P}(z)\int_0^1 dy\ y^{n-1} f(y) \nonumber \\
&= \frac{\alpha_s}{2 \pi} \mathcal{P}(n)\ f(n) \equiv \gamma(n)\ f(n),
\end{split}
\ee

where $\gamma(n)_{ij}$ (with parton indices explicit) are known as the anomalous dimensions, and are calculable order by order in perturbation theory. 
\subsection{Hadroproduction}

At the LHC most processes involve the interaction of two protons. Hadron-hadron collisions can be approached in much the same way as DIS, but instead the process is like in Fig. \ref{fig:hadroproduction}. Because two protons are involved the expression for the cross section is the natural extension of the DIS case (Eqn.~\ref{eqn:disfact}):
\be
\sigma = \sum_{i,j} \int dx_1 dx_2 f_i(x_1, \mu_F^2) f_j(x_2, \mu_F^2) \hat{\sigma}_{ij}\bigg(x_1, x_2, \frac{Q^2}{\mu_F^2},...\bigg).
\ee

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{../diagrams/hadroproduction.pdf}
\caption{\label{fig:hadroproduction}Factorisation in hadron-hadron collisions.}
\end{figure}

\subsection{Sum rules}

Although PDFs may seem at first sight to be totally unknown there are some theoretical observations which
we can use to constrain their form.
These are known as the ``sum rules''~\cite{pinkbook}. Intuitively, adding up all the momenta of the partons must equal the
momentum of the proton. This enforces the condition

\beq
  \int_0^1 dx \sum_i x f_i(x,Q^2) = 1.
\eeq

The other thing we know about the
proton is that it is made up of two up and one down
``valence'' quarks. Any other quarks must be pair-produced from the sea, and
therefore come with an antiquark of the same flavour. So we can normalise the PDFs using the expressions: 

\begin{subequations}
 \beq
   \int_0^1 dx \big( f_u - f_{\bar{u}} \big) = 2;
 \eeq
 \beq
   \int_0^1 dx \big( f_d - f_{\bar{d}} \big) = 1;
 \eeq
 \beq
   \int_0^1 dx \big( f_q - f_{\bar{q}} \big) = 0, \qquad q = s, c, t, b.
 \eeq
\end{subequations}

Note that these conditions require that the PDFs are integrable. 


\section{Methodological background}

In this section we review the necessary background for PDF determination within the NNPDF \cite{nnpdf} framework. The results in this thesis are based on two different versions of this: NNPDF3.1 and NNPDF4.0. First we touch on the experimental and theoretical inputs to PDF fits, which are common to both versions. Then we outline the NNPDF3.1 framework, which was used to generate the results in Chapters \ref{chapter:mhous} and \ref{chapter:correlations}. We summarise the NNPDF fitting strategy, and detail information on neural networks specific to this context. Finally, we explain the main differences between this and NNPDF4.0, which was used to generate the results in Chapter \ref{chapter:nuclear}.

\subsection{Experimental and theoretical input}

NNPDF uses a variety of experimental data from a number of particle colliders, including those based at CERN \cite{cern} and Fermilab \cite{fermilab}. These are observables such as cross sections, differential cross sections and structure functions. Fig. \ref{data} is a plot of the $(x,Q^2)$ range spanned by the datasets in the upcoming NNPDF4.0 release, with those in NNPDF3.1~\cite{Ball:2017nwa} shown without a black edge. Much of the data are from DIS processes, which are crucial in determining PDF functional form, but in recent years increasingly more LHC collider data has been added including $t\bar{t}$ production and high energy jets. For a full review of the data in both NNPDF3.1 and NNPDF4.0 see \cite{EmanueleTalk}.

\begin{figure}
\centering
\includegraphics[width=15cm]{background/kinplot.pdf}
\caption{Plot of the $(x,Q^2)$ range spanned by data included in the upcoming NNPDF4.0 NLO fit. Datasets in NNPDF3.1 are those without the black edge.}
\label{data}
\end{figure}

Theoretical predictions of the corresponding parton-level observables are computed using external codes~\cite{jr:mcfm,Alwall:2014hca,Catani:2009sm,Gavin:2010az,Catani:1996vz}. These are converted to higher orders of perturbation theory as necessary using QCD and electroweak correction factors (known as ``$c$'' or ``$k$" factors). They are then combined with DGLAP evolution kernels, which evolve PDFs from an initial reference energy scale to the energy scale of each experiment using the DGLAP equations (Eqn. \ref{eqn:DGLAP}). This evolution is done using \texttt{APFEL}~\cite{Bertone:2013vaa}. 


\subsection{Experimental uncertainties}
\label{sec:expuncs}

Experimental uncertainties are described using a covariance matrix, $C_{ij}$, which gives the uncertainties and correlations between each of the data points $i,j = 1,...,N_{dat}$. It encapsulates the total breakdown of errors, $\sigma$, and can be constructed using uncorrelated errors ($\sigma_i^{uncorr}$), and  additive ($\sigma_{i,a}$) and multiplicative  ($\sigma_{i,m}$) correlated systematic errors (more on these below):
\beq
  C_{ij} = \delta_{ij}\sigma_i^{uncorr}\sigma_j^{uncorr} + \sum_a \sigma_{i,a}\sigma_{j,a} +
  \bigg( \sum_m \sigma_{i,m}\sigma_{j,m} \bigg) D_i D_j,
\label{eq:expcov}
\eeq
where $D_i$ are the experimental data values.

Structurally, the uncorrelated statistical uncertainties appear down the diagonal and these are what we would recognise intuitively as the statistical error ``on a data point". However, correlated
systematic uncertainties can also appear on the off-diagonals. Correlated uncertaintes include
those which link multiple data points, for example systematic uncertainties from a particular
detector which will affect all of its data in a similar way.

Systematic uncertainties further divide into two types, ``additive'' and ``multiplicative''.
Additive systematics are perhaps a more familiar type of error, and are independent of the
datapoint values themselves. On the other hand,  multiplicative systematics depend on the measured values. In the context of particle physics 
experiments, a common example is total detector luminosity. This is because recorded cross
sections are dependent on the luminosity of the detector; a higher luminosity means more
collisions will take place so the measured cross section will be greater.

Fig. \ref{fig:expcovmat} is an example of an experimental covariance matrix for data included in an NNPDF fit. The data are grouped according to what type of process the interaction belongs to (DIS charged current (CC) and neutral current (NC), Drell-Yan (DY), jets and top production). Systematic correlations within experiments are responsible for off-diagonal contributions, and these are mostly positive correlations but there is some anticorrelated behaviour in DIS CC, as a result of data in different kinematic regimes. 

\begin{figure}
\centering
\includegraphics[width=15cm]{background/exp_covmat.pdf}
\caption{An example of an experimental covariance matrix for data included in an NNPDF fit. The data are grouped according to what type of process the interaction belongs to (DIS charged current (CC) and neutral current (NC), Drell-Yan (DY), jets and top production).}
\label{fig:expcovmat}
\end{figure}

The covariance matrix can be used to define the $\chi^2$ figure of merit, 
\be
\label{eqn:chi2}
\chi^2 = \frac{1}{N_{dat}} (D_i-T_i) C_{ij}^{-1} (D_j-T_j),
\ee
which measures how good the fit is between the experimental data $D_i$ with associated covariance $C_{ij}$, and theory predictions $T_i$. In practice, this definition is subject to d'Agostini bias \cite{DAgostini:1993arp} due to the presence of normalisation uncertainties. To avoid this, NNPDF employ the iterative $t0$ procedure \cite{Ball:2009qv} whereby $D_i$ in Eqn. \ref{eq:expcov} are replaced initially with the predictions from a baseline fit, and the covariance matrix is iterated concurrently with preprocessing. 


\subsection{NNPDF fitting strategy}

There are a number of groups currently active in carrying out proton PDF fits including MSHT~\cite{Bailey:2020ooq}, CTEQ~\cite{Hou:2019efy}, NNPDF~\cite{nnpdf}, HERAPDF/xFitter~\cite{CooperSarkar:2011aa} and ABM~\cite{Alekhin:2019ntu}. 
\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{background/eleanor_strategy.pdf}
\caption{NNPDF general strategy. Image credit: Eleanor Conole.}
\label{fig:generalstrategy}
\end{figure}
The work in this thesis has been carried out in the framework developed by the NNPDF collaboration, so we will concentrate on this fitting strategy, which is summarised in Fig.~\ref{fig:generalstrategy}. There are two main features which differ from other fitting collaborations'~\cite{Forte:2002fg}. These are:
\begin{enumerate}
\item  The use of Monte Carlo approach to error analysis;
    \item  Fitting using artificial neural networks.
\end{enumerate}

In the following sections we will provide an overview of these aspects, which can be found in more detail in \cite{Ball:2010de, Ball:2012cx, Ball:2017nwa}.

\subsection{Monte Carlo approach}
The uncertainties in the functional form of PDFs come as a direct consequence of the uncertainties in the experimental and theoretical inputs. In order to propagate experimental uncertainties through to the PDFs, NNPDF represent the experimental data (central values and uncertainty distribution) as a Monte Carlo ensemble. This is a set of $N_{rep}$ Monte Carlo ``replicas" which, given high enough replica number, have a mean value equal to the data central value and covariance equal to the experimental covariance. Fig. \ref{fig:MC} is a schematic illustrating the generation of these ``pseudodata", $D^{(k)}$, $k=1,...,N_{rep}$.
\begin{figure}
\centering
\includegraphics[width=0.48\linewidth]{background/mcreps_upper.png}
\includegraphics[width=0.48\linewidth]{background/mcreps_lower.png}
\caption{Generation of Monte Carlo replicas of pseudodata from data with uncertainties. Left: experimental data; right: five Monte Carlo replicas.}
\label{fig:MC}
\end{figure}
They are generated using Gaussian random numbers $n_a^{(k)}$ and $\hat{n}_m^{(k)}$:
\be
D^{(k)} = (D^0 + \sum_a n_a^{(k)} \sigma^a) \prod_m (1 + \hat{n}_m^{(k)}\sigma^p),
\ee
where $D^0$ is the experimental data value, and $\sigma^a$ and $\sigma^m$ are the additive and multiplicative uncertainties discussed in Sec. \ref{sec:expuncs}. Sometimes uncertainties are asymmetric, and in this case we adjust the data value such that the uncertainties are made symmetric. Explicitly, the pseudodata replicas satisfy the relations:
\be
\langle D_i^{(k)} \rangle = D_i^0; \qquad \langle (\langle  D_i^{(k)} \rangle -D_i^0)(\langle  D_j^{(k)} \rangle-D_j^0)\rangle = C_{ij},
\ee
in the limit of $N_{rep} \to \infty$, where the notation $\langle \cdot \rangle$ denotes the mean over replicas. Fig. \ref{fig:datarepchorus} shows the distribution of pseudodata for a single data point.

\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{background/datarepchorus.png}
\caption{Histogram of the distribution of 100 pseudodata replicas for a single data point with 8.89\% uncertainty, normalised to $D^0$. The purple line is the mean value $\langle D^{(k)} \rangle$, which is equal to $D^0$ (black line) to arbitrary precision.}
\label{fig:datarepchorus}
\end{figure}
Once the pseudodata have been generated, each replica, $D^{(k)}$, is fitted separately to the theoretical predictions, $T_i[f_q^{(k)}]$, which depend on the PDF replicas, $f_q^{(k)}$ (where $q$ runs over the fitted flavours: $g$, $u$, $d$, $s$, $c$, $\bar{u}$, $\bar{d}$, $\bar{s}$, $\bar{c}$). This is done by fitting the PDFs to minimise a target error function based on the $\chi^2$:
\be 
\label{eq:chi2background}
\chi^{2\ (k)} = \frac{1}{N_{dat}}\sum_q \sum_{i,j=1}^{N_{dat}} (D_i^{(k)}-T_i[f_q^{(k)}])C(t_0)_{ij}^{-1}(D_j^{(k)}-T_j[f_q^{(k)}]).
\ee
Here $C(t_0)$ is the t0 covariance matrix, which is Eqn.~\ref{eq:expcov} with $D_i \to t_{0\; i}$, where $t_0$ are theory predictions from a similar previous fit. This is to remove d'Agostini bias~\cite{DAgostini:1993arp, Ball:2009qv}. Doing this fit then results in a PDF set of each flavour. These act as a Monte Carlo parametrisation of the PDFs (for example, Fig. \ref{fig:replicas}).  This means that the PDFs and their errors can be extracted by taking the means and standard deviations over the ensemble. The final PDFs are made publicly available as downloadable files on the LHAPDF website \cite{lhapdf, Buckley:2014ana}. 

\begin{figure}[H]
\centering
    \includegraphics[width=0.7\textwidth]{background/Qs0_NNPDF31NLO_plot_pdfreplicas_d_v.png}
\caption{Monte Carlo replicas for the down valence quark PDF NNPDF3.1 at NLO. } \label{fig:replicas}
\end{figure}

\subsection{Neural networks}

Inspired by how the brain processes information, in machine learning neural networks are graphs of connected nodes. They are trained by example, so
have the capability to learn a PDF's functional form given a set of data. Using neural networks rather than specific functional forms allows us to avoid the theoretical bias which goes into selecting such a functional form. The layout, or ``architecture", consists
of input layers, hidden layers and output layers. Nodes can be either input nodes or activation nodes, the latter of which have an associated activation function which is applied to their output. 
\subsubsection{Neural networks in NNPDF3.1}
Fig. \ref{fig:nn} depicts the architecture used in NNPDF3.1. This is a ``2-5-3-1" archiecture, where the numbers refer to the number of nodes in each layer. 
\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{../diagrams/neuralnet.pdf}
\caption{Schematic depiction of the 2-5-3-1 architecture of an artificial neural network of NNPDF3.1. Here $\xi_1^{(1)}$ and $\xi_2^{(1)}$ are the variables $x$ and $\ln (1/x)$ respectively. \label{fig:nn}}
\end{figure}
It is a ``multilayer perceptron", meaning the graph is fully connected, and it is a feed-forward; information can only be passed in one direction through the layers (from 
input to output). The two inputs are $x$ and $\ln (1/x)$, and the output, $f$ or $\xi^{(4)}$, is the PDF at the parametrisation scale, $Q_0$. In this network the output of a node in the $l^{th}$ layer is given by
\beq
  \xi_i^{(l)} = g \bigg( \sum_j^{inputs} \omega_{ij}^{(l)} \xi_j^{(l-1)} + \theta_i^{(l)} \bigg)
\eeq
where the $\omega$s and $\theta$s are ``weights'' and ``thresholds''; parameters to be minimised
with respect to.  $g$ is an ``activation function'' which is set to
\beq
  g(z) =
\begin{cases}
 \frac{1}{1 + \exp (-z)} &\text{for hidden layers}\\
  z &\text{for the output layer}.
\end{cases}
\eeq
The choice of this sigmoid activation function for the hidden layers allows sufficient non-linear freedom in the functional form, and the linear activation function for the output layer ensures the range of the PDFs is not restricted to [0,1].

The training of the neural networks is implemented using a ``genetic algorithm"~\cite{DelDebbio:2007ee} (CMA-ES), so-called because of the introduction of mutation to the fitting parameters. This additional degree of randomness helps to avoid getting stuck in local minima. In practice, this involves ``mutating" some chosen fraction of the thresholds, $\theta$, by perturbing them at random.

\subsection{Parametrisation, preprocessing and postprocessing}
A scale of $Q=1.65$ GeV is chosen to parametrise the PDFs at, and then they can be determined at any other scale by evolution using the DGLAP equations (Eqn. \ref{eqn:DGLAP}). The PDFs are fitted whilst parametrised in a ``fitting basis" of eight combinations of flavours, to help convergence \cite{Ball:2014uwa}, defined:
\begin{itemize}
\item $g$;
\item $\Sigma \equiv \sum_{u,d,s} q_i + \bar{q}_i$;
\item $T_3 \equiv u - d$;
\item $T_8 \equiv u + d - 2s$;
\item $V \equiv \sum_{u, d, s} q_i - \bar{q}_i$;
\item $V_3 \equiv \bar{u} - \bar{d}$;
\item $V_8 \equiv \bar{u} - \bar{d} - 2 \bar{s}$;
\item $c$.
\end{itemize}
Since the form of the neural networks ($N_i(x)$) is determined by training on experimental data, the output is not meaningful outwith the data region. The functional form of the PDFs in this so-called ``extrapolation region" is in practice fixed through enforcement of the known high and low $x$ behaviour via ``preprocessing"; the PDFs are parametrised as:
\beq
  f_i(x) = A_i x^{-\alpha_i} (1-x)^{\beta_i} N_i(x).
\eeq
$A_i$ are normalisation coefficients, which are fixed at each iteration of the fit. There are seven of these initial coefficients, three of which are set by the valence sum rules and one by the momentum sum rule. The other three are initially set to 1; see \cite{NNPDF:2014otw} for more information. The powers $\alpha_i$ and $\beta_i$ are fitted parameters determined by iteration from one fit to the next. This preprocessing has the effect that the PDFs approach 0 at large $x$, and generally grow at small $x$. This is because the probability of the existence of a parton is generally small at high $x$ and larger with decreasing $x$ outwith the data region.

Postprocessing is also applied to the PDF replicas to remove those which don't satisfy certain quality conditions. That is, where the target error function or arc-length of the replica is more than four standard deviations outwith the mean, or where the positivity of the resulting cross-sections is not satisfactorily maintained. 
\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{background/eleanor_overlearning.pdf}
\caption{\label{fig:overlearning}Overlearning: the data points (black dots) fluctuate around the linear underlying law (black line), but the neural network continues to minimise the error function until it passes through every data point (blue curve), fitting the noise in the data. Image credit: Eleanor Conole.}
\end{figure}

\subsection{Cross validation}
Neural networks are effective at learning the functional form which underlies data. Sometimes, if there are more degrees of freedom in the PDF than in the data, they can be ``too effective", picking up not just the underlying law but also the noise. This is known as ``overlearning" (see Fig. \ref{fig:overlearning} for an example).
\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{background/eleanor_lookback.pdf}
\caption{\label{fig:crossvalidation}Cross validation with the lookback method. Image credit: Eleanor Conole.}
\end{figure}

%\begin{figure}[h]
%\centering
%\includegraphics[width=\textwidth]{background/trvalchi2.pdf}
%\caption{\label{fig:trvalchi2}Comparing the training and validation $\chi^2$s %for the 100 replicas (green circles) of a PDF fit. The red square gives the average.}
%\end{figure}

To circumvent this problem, the data are split into a training and a validation set. The training data are used to optimise the neural network, and the validation data are used to test the network output, in a process known as ``cross validation". As training epochs elapse, the target error function compared to both the training and validation data should decrease as the network learns the underlying law. At some point, however, the network will begin to learn the noise in the training data, at which point the training error function will continue to decrease, but the validation error function will stop decreasing and start to increase again. In NNPDF3.1 we determine the optimum fit using the ``lookback" method (Fig.~\ref{fig:crossvalidation}), where after training the model corresponding to the minimum in the validation error function is selected.

\section{NNPDF4.0}
The earlier parts of this section describe the methodology for the NNPDF3.1~\cite{Ball:2017nwa} release, on which Chapters \ref{chapter:mhous} and \ref{chapter:correlations} are based. However, at the time of writing this thesis a new release, NNPDF4.0, is being launched. The work in Chapter~\ref{chapter:nuclear} is based on this methodology, and so we briefly explain the main developments between NNPDF3.1 and NNPDF4.0. For more information see \cite{PDF4LHC, EmanueleTalk}, and the future NNPDF4.0 paper.

\subsection{Methodology}
NNPDF4.0 heralds some significant methodological changes to the NNPDF procedure, the most important of which we will outline here. Perhaps most strikingly, the architecture of the neural network is changed~\cite{Carrazza:2019mzf} from that in Fig.~\ref{fig:nn}, which is implemented separately for each flavour, to a single neural network with flexible architecture. The implementation is via \texttt{Keras}~\cite{keras} and \texttt{Tensorflow}~\cite{tensorflow}, and the optimisation is with an inbuilt gradient descent algorithm rather than the genetic algorithm in NNPDF3.1. The stopping criterion follows a patience method where the fit stops once the minimiser is stable for a set length of time. All of these parameters, including architecture are determined via hyperoptimisation with k-folding, which is a process to determine the best combination for stability and performance.

Additionally, positivity of PDF replicas is now strictly enforced rather than them being allowed to be negative within a threshold. This is following a recent study~\cite{Candido:2020yat} which showed that $\overline{MS}$ PDFs are strictly positive. 

\subsection{Theory developments}
There are two main theory developments between NNPDF3.1 and NNPDF4.0. One is the treatment of nuclear and deuteron data using an additional uncertainty. This forms the basis of Chapter~\ref{chapter:nuclear} so we will not discuss it further here.

The other is the inclusion of NLO electroweak (EW) corrections. This consists of QED corrections to the DGLAP evolution as well as NLO EW corrections for a variety of processes; wherever EW corrections are available they are added to increase precision. This is very important as EW corrections can be up to $\sim 20\%$~\cite{Hollik:2004dz} in some regions. The corrections are provided as interpolation grids via the \texttt{PineAPPL} library~\cite{Carrazza:2020gss}.  

\subsection{Validation of PDFs}
The effectiveness of the fitting methodology has traditionally  been tested using closure tests~\cite{Ball:2014uwa}, which use a separate PDF to create proxy known ``true values". This procedure has been updated, whereby fits are carried out to many proxy PDF replicas, and the bias and variance of the results are compared. This is made possible by the significantly increased speed of the new fitting methodology~\cite{Carrazza:2019mzf}.

\subsection{New data}
NNPDF4.0 also includes a large number of additional datasets, including many from the 13 TeV run at the LHC (see Fig.~\ref{data}). What's more, many of the existing datasets' implementation has been improved~\cite{EmanueleTalk}.

