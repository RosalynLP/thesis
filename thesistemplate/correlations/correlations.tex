\chapter{Making predictions using PDFs with theoretical uncertainties}

Earlier in this thesis we have discussed the importance of theoretical uncertainties, and how to include them in PDFs. We have also produced PDFs including MHOUs (see Chapter~\ref{chapter:mhous}) and deuteron and nuclear uncertainties (see Chapter~\ref{chapter:nuclear}), which are freely available. In the future, PDFs with theory uncertainties will become the norm, and will be used widely to make theoretical predictions for observables by convoluting them with parton-level hard cross sections. 

When making predictions for hadronic observables there are two sources of uncertainty: the hard cross section and the PDF. Typically, MHOUs for the former are estimated using scale variation, and these are added in quadrature to the latter. When the PDFs themselves also include MHOUs, we can think of the two scale evolutions considered being $Q_0 \to Q_{dat}$ and $Q_0 \to Q_{pred}$, where $Q_0$ is the PDF parametrisation scale, $Q_{dat}$ is the scale of data in the PDF and $Q_{pred}$ is the scale the prediction is made at. Then each of these sources has both a contribution from renormalisation scale variation and from factorisation scale variation.  The PDFs themselves contain a wealth of data from various processes, so when making a prediction for a process which is included in the PDFs there will invariably be correlations between the renormalisation scale variation in the PDF and that in the hard cross section. Even if the process is a new one, for example Higgs production, correlations due to factorisation scale variation will always be present. Simply combining the two sources of uncertainty in quadrature will miss these correlations and lead to an inflation in overall uncertainties. 

This issue was explored in detail in \cite{Harland-Lang:2018bxd}, noting that PDFs are a tool to express one observable in terms of others. This can be realised exactly in the simple case of fully correlated factorisation scale variation for non-singlet structure functions. Here the PDF can be eliminated entirely, and it is manifest that there is only one independent scale ($Q_{dat} \to Q_{pred}$), rather than two, with the MHOUs cancelling to a large degree. If MHOUs were used in both the PDF and the hard cross section in this case, it would amount to ``double counting". It was also shown that correlations existed for renormalisation scale variation, albeit less strongly.

When using PDFs in predictions we include both sources of scale variation in an uncorrelated way, and so miss the MHOU cancellation and corresponding reduction in uncertainties. As noted in \cite{MHOUbig}, this is a consequence of PDFs being universal. We cannot reconstruct the full data and MHOUs from the PDFs alone as information is lost in the fitting process; one set of PDFs could arise from many different data possibilities. However, if we want PDFs to be useful in a wide range of predictions, universality is required and so we must live with this loss of correlation.

Fortunately, the increase in PDF uncertainties due to MHOUs is small, and the effect is mostly realised in changes to the central value as the fit is rebalanced by changes to the weighting of different data. In indicative cases explored in Chapter 7 of \cite{MHOUbig}, it was seen that the PDF uncertainty is consequently much smaller than the MHOU on the hard cross section. When combining these two in quadrature, the effect of missing correlations will therefore likely be small, and so the overestimate of uncertainty will be small. It was argued that a small overestimate of uncertainty is better than neglecting MHOUs altogether. However, accounting for these correlations is desirable in order to keep uncertainties as low as possible in the current high precision era.

In this chapter we investigate correlations between PDF MHOUs and hard cross section MHOUs when making predictions. Although we focus on MHOUs, the analysis extends naturally to all sources of theory uncertainty. We develop a method for algebraically determining these correlations, and show how to include them when making a prediction. This is a complicated problem so we proceed incrementally:

\begin{itemize}
\item In Sec.~\ref{sec:p1} we show how a theoretical uncertainty can be reformulated in terms of a nuisance parameter, which holds the key to the propagation of uncertainties. We consider as a starting point an exact fit, showing that the experimental data partially decorrelate the theory uncertainties in the fit and those in the prediction.
\item In Sec.~\ref{sec:p2} we move on to a model where the data are fitted using just one parameter. Here the correlations lead to a shift in the theoretical predictions, but the fitting of the data leads to loss of information and consequently further decorrelation.
\item In Sec.~\ref{sec:p3} we extend this analysis to a multi-parameter fit with multiple theory uncertainties, and then to a PDF fit, where the PDFs are continuous functions with a functional uncertainty. We show that this functional uncertainty leads to a further decorrelation in the MHOUs. 
\item In Sec.~\ref{sec:p4} we present numerical results comparing this procedure with the naive approach of adding the uncorrelated contributions in quadrature, in the context of the NLO global fit with MHOUs discussed in Chapter \ref{chapter:mhous}. We make predictions including MHOUs for repetitions of all the experiments already in the fit (so-called ``autopredictions"). We then investigate the scenario of a prediction for a process already in the fit (top production), and for a new process (Higgs production), and show that including these correlations leads to a shift in the central value of the prediction that is well within the total uncertainty, and that the decorrelation of the MHOUs is almost complete. We see that in practice adding the two contributions in quadrature and keeping them uncorrelated (the ``conservative" prescription of Chapter \ref{chapter:mhous}) is remarkably accurate. Having said that, the methodology for including full correlations is now available. Finally, we suggest a novel way of improving theoretical predictions, using ths correlated shift.
\item In Sec.~\ref{sec:p5} we provide a summary.
\end{itemize}

\section{Predictions with correlated theory uncertainties}
\label{sec:p1}

We saw in Chapter \ref{chapter:thuncs} that to include theory uncertainties in a fit all you need to do is add a theory covariance matrix, $S_{ij}$, to the experimental covariance matrix $C_{ij}$. Recall that $i,j = 1, \dots, N_{dat}$ run over data points. The only assumptions underlying this result are that all uncertainties are Gaussian, and that the theory uncertainties are independent of the experimental data. Since Gaussian experimental uncertainties are already assumed in NNPDF's framework, these assumptions are very reasonable. We can express the result as the conditional probability
\be
\label{eqn:ptd}
P(T|D) \propto \exp \bigg( -\frac{1}{2} (T-D)^T(C+S)^{-1}(T-D) \bigg).
\ee
Recall that both $C$ and $S$ are real and symmetric, that $C$ is positive definite and that $S$ is positive semidefinite and will generally possess many zero eigenvalues. In a fit we determine $T$ from $D$ by maximising $P(T|D)$, which amounts to minimising
\be
\label{eqn:chi2}
\chi^2 =  (T-D)^T(C+S)^{-1}(T-D)
\ee
with respect to the free parameters which characterise the theory prediction.

In this section we start off by considering one single source of fully correlated theory uncertainty, so that
\be 
S = \beta \beta^T,
\ee
where $\beta$ are real and non-zero. 

\subsection{Nuisance parameters}
We can model the theory uncertainty as a fully correlated shift in the theory prediction:
\be 
T \to T + \lambda \beta,
\ee 
where $\lambda$ is a nuisance parameter characterising the scale of the shift. We will now show that this will lead us to Eqn.~\ref{eqn:ptd}. Firstly, assuming Gaussian experimental uncertainties, we can write
\be 
\label{eqn:nuisptd}
P(T|D\lambda) \propto \exp \bigg( -\frac{1}{2} (T + \lambda \beta -D)^T C^{-1}  (T + \lambda \beta -D) \bigg).
\ee
Using Bayes' Theorem, 
\be 
\label{eqn:bayes}
P(T|D\lambda) P(\lambda |D) = P(\lambda | TD)P(T|D).
\ee
We want to find $P(T|D)$, so we need an expression for the prior for $\lambda$, $P(\lambda |D) = P(\lambda)$, where we assume that the theory uncertainty is independent of the experimental data. We choose a unit-width Gaussian centred on zero, 
\be 
\label{eqn:lambdaprior}
P(\lambda) \propto \exp \bigg(-\frac{1}{2} \lambda^2 \bigg).
\ee
Marginalising over $\lambda$, Eqn.~\ref{eqn:nuisptd} becomes
\be 
\label{eqn:ptd2}
P(T|D) \propto \int d\lambda \exp \bigg( -\frac{1}{2} \bigg[ (T + \lambda \beta -D)^T C^{-1} (T + \lambda \beta -D) + \lambda^2 \bigg] \bigg).
\ee
We can evaluate the term in $[ \cdot ]$ by remembering $S= \beta \beta^T$, introducing the variable
\be 
\label{eqn:z}
Z \equiv (1 + \beta^T C^{-1} \beta)^{-1},
\ee
and completing the square:
\be 
\begin{split}
[ \cdot ] &= (T-D)^T C^{-1}(T-D) + (T-D)^T C^{-1} \lambda \beta + \lambda \beta^T C^{-1} (T-D) \\ &+ \lambda \beta^T C^{-1} \lambda \beta + \lambda^2 \\
& = (T-D)^T C^{-1}(T-D) + (T-D)^T C^{-1} \lambda \beta + \lambda \beta^T C^{-1} (T-D) + \lambda^2 Z^{-1} \\
&=  Z^{-1}(\lambda + Z \beta^T C^{-1} (T-D))^2 \\ &- Z(\beta^T C^{-1} (T-D))^2 + (T-D)^T C^{-1}(T-D) \\
&= Z^{-1}(\lambda + Z \beta^T C^{-1} (T-D))^2 \\ &+ (T-D)^T (C^{-1} - Z C^{-1} S C^{-1})(T-D).
\end{split}
\ee
Finally, we can use the Sherman-Morrison formula and Eqn.~\ref{eqn:chi2} to write this as
\be 
\begin{split}
[ \cdot ] &= Z^{-1}(\lambda + Z \beta^T C^{-1} (T-D))^2 + (T-D)^T (C + S)^{-1}(T-D) \\
&= Z^{-1}(\lambda + Z \beta^T C^{-1} (T-D))^2 + \chi^2 \\
&\equiv Z^{-1}(\lambda - \bar{\lambda}) + \chi^2,
\end{split}
\ee
where we have defined $\bar{\lambda} = Z \beta^T C^{-1} (D-T)$. Plugging this back into Eqn.~\ref{eqn:ptd2}, we get
\be  
\begin{split}
P(T|D) &\propto \int d\lambda \ e^{-\frac{1}{2} \chi^2} \exp \bigg( -\frac{1}{2} Z^{-1} (\lambda - \bar{\lambda})^2 \bigg) \\
&\propto e^{-\frac{1}{2} \chi^2},
\end{split}
\ee
which is Eqn.~\ref{eqn:ptd}. The advantage of this approach is that we can also get the posterior distribution for $\lambda$ (after fitting using $D$ and $T$), by using Bayes' Theorem (Eqn.~\ref{eqn:bayes}):
\be 
\begin{split}
\label{eqn:lambdaposterior}
P(\lambda |TD) &= \frac{P(T|D\lambda) P(\lambda)}{P(T|D)} \\
&= \exp \bigg( -\frac{1}{2} \bigg[ (T+ \lambda \beta -D)^T C^{-1}(T+ \lambda \beta -D) + \lambda^2 - \chi^2 \bigg] \bigg) \\
& = \exp \bigg( -\frac{1}{2} Z^{-1}(\lambda - \bar{\lambda})\bigg),
\end{split}
\ee
where we recognised the similarity between the exponent here and in Eqn.~\ref{eqn:ptd2}. So the effect of the fit is to shift the centre of the distribution from 0 $\to \bar{\lambda}$, and the width from 1 $\to Z$. Note that from the definition of $Z$ (Eqn.~\ref{eqn:z}), 
\be 
0 \le Z \le 1,
\ee
so the theory uncertainty is always reduced when information on $D$ is added. Fig.~\ref{fig:lambdadistribs} gives a sketch of this effect.  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[H]
  \begin{center}
      \includegraphics[width=0.5\textwidth]{correlations/plots/lambdapriorpost.png}
    \caption{Sketch of the prior (Eqn.~\ref{eqn:lambdaprior}) and posterior (Eqn.~\ref{eqn:lambdaposterior}) distributions for $\lambda$. Adding information shifts the distribution and reduces the width. \label{fig:lambdadistribs}}
    
  \end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Decorrelation}
We will now test out this formalism in the case of a ``perfect fit". This is one where the theory inputs to the fit are flexible enough to fit the data exactly. It is not a especially realistic situation in the context of PDF fits, but will serve as a useful foundation on which to model more complex situations. 

In this perfect fit $P(T|D)$ (Eqn.~\ref{eqn:ptd2}) is maximised when $T=D$, which corresponds to $\chi^2 = 0$. The data and theory are therefore fully correlated, and the expectation value, $E[\cdot]$, and covariance, $\Cov[\cdot]$, of the theory are
\be
\label{eq:perfectfit}
E[T] = D,\qquad \Cov[T] = \Cov[D] = C+S.
\ee
Looking back at the definition of $\bar{\lambda}$, we can see that after the fit $\bar{\lambda}=0$, and so Eqn.~\ref{eqn:lambdaposterior} is independent of $T$. The posterior for $\lambda$ has 
\be 
\label{eq:meanvarlam}
E[\lambda] =0,\qquad \Var[\lambda] = Z.
\ee
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[H]
  \begin{center}
      \includegraphics[width=0.5\textwidth]{correlations/plots/lambdaperfectfit.png}
    \caption{Sketch of the prior (Eqn.~\ref{eqn:lambdaprior}) and posterior (Eqn.~\ref{eqn:lambdaposterior}) distributions for $\lambda$ in the case of a perfect fit. The additional information from $D$ reduces the uncertainties but there is no shift as the theory fit the data exactly. \label{fig:lambdaperfect}}
   
  \end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Fig.~\ref{fig:lambdaperfect} shows a sketch of the effect on the distribution of $\lambda$. What happens if we fit $T$ to $D$ and then use the results of this fit to make predictions for $T$? We can call this an ``autoprediction", because we are making predictions about the theory included in the fit itself. In effect this is like making theory predictions for another set of experimental results where the settings tie up exactly with those already included in the fit. In this scenario, the uncertainty is reduced as the experimental data have added information. Remembering that we can write $T(\lambda) = T + \lambda \beta$, we find
\be
\label{eq:perfectcovT}
\begin{split}
&E[T(\lambda)] = E[T]=D \\ &{\Cov}[T(\lambda)] 
= {\Cov}[T] + \Var[\lambda] \beta\beta^T = C+S+ZS.
\end{split}
\ee
There are three contributions to the covariance: the experimental uncertainty in the data, the theory uncertainty in the fit, and the theory uncertainty in the prediction. Note that the theory uncertainty in the prediction is reduced by a factor $Z={\rm Var}[\lambda]$, because of the correlation between this and the theory uncertainty in the fit. Note that:
\begin{itemize}
\item ignoring correlations amounts to setting $Z=1$ and we get ${\Cov}[T(\lambda)] = C + 2S$. This is the ``conservative prescription" of Chapter.~\ref{chapter:mhous}.
\item adopting the approach of \cite{Harland-Lang:2018bxd} and assuming full correlations amounts to setting $Z=0$ and we get ${\Cov}[T(\lambda)] = C + S$, so the conservative prescription double counts with respect to this.
\end{itemize}
In reality, the situation is somewhere between these two; we see a partial decorrelation due to the presence of experimental uncertainties.

To get a better sense of the size of decorrelation, let's consider a simple model where the experimental covariance matrix is diagonal and the theory uncertainty is fully correlated. Writing $\beta = s e$, where $s$ is the size of correlated theory uncertainty and $e^Te=1$, we have
\be
\label{eq:modelCS}
C = \sigma^2 \mathbb{I},\qquad S = s^2 e e^T,
\ee
where $\sigma$ is the per-point uncertainty. Then we can evaluate:
\be
\begin{split}
\label{eq:modelCplusSinv}
(C+S)^{-1} &= \frac{1}{\sigma^2}\left(1-\frac{s^2}{\sigma^2+s^2}e e^T\right); \\
Z &= (1+s^2/\sigma^2)^{-1}.
\end{split}
\ee
so we see that the extent of decorrelation depends on the ratio $s^2/\sigma^2$:
\begin{itemize}
\item as $\sigma^2/s^2 \to 0$ we tend to the case of no experimental uncertainties, as considered in \cite{Harland-Lang:2018bxd}, and $Z \to 0$.
\item as $s^2/\sigma^2 \to 0$, $Z \to 1$, which is the conservative prescription.
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[h]
  \begin{center}
      \includegraphics[width=0.5\textwidth]{correlations/plots/plot1.pdf}
    \caption{ For a perfect fit within the model Eqn.~\ref{eq:modelCS}.  Comparing the ratios to the conervative prescription of the decorrelated ($R$, Eqn.~\ref{eq:modelR}) and correlated ($R_c$, Eqn.~\ref{eq:modelRc}) autoprediction covariance matrices.  \label{fig:plot1}}
  \end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Now consider the covariance matrix of autopredictions. In the conservative case this is $C + 2S$, which in the direction of $e$ is $\sigma^2 + 2s^2$. In any direction orthogonal to $e$ it is just $\sigma^2$ because there are no theory uncertainties in these directions. We can calculate the ratio of the true covariance matrix to the conservative one within the model in Eqn.~\ref{eq:modelCS}. This ratio is
\bea
R = \frac{e^T(C+S+ZS)e}{e^T(C+2S)e} &=& \frac{\sigma^2+(1+Z)s^2}{\sigma^2+2s^2}\nn\\
&=& 1-\frac{s^4}{\sigma^4+3\sigma^2 s^2 + 2s^4}\nn\\
&\sim&  \begin{cases}{1 -\smallfrac{ s^4}{\sigma^4} \qquad\mbox{for $s^2\ll\sigma^2 $;}}\\{ \half(1+\smallfrac{3\sigma^2}{2s^2})\qquad \mbox{for $s^2 \gg \sigma^2$.}}\end{cases} 
\label{eq:modelR}
\eea
We can see that
\begin{itemize}
\item when experimental uncertainties dominate then the theory uncertainties are almost entirely decorrelated. In this case the conservative prescription is almost exact; only a slight overestimate.
\item when theory uncertainties dominate then there is very little decorrelation, and the conservative prescription double counts the theory uncertainties.
\end{itemize}
When we're between these extremes, we can look at the difference between this ratio and that for the fully correlated case, which is
\bea
R_c = \frac{e^T(C+S)e}{e^T(C+2S)e} &=& 1 -\frac{s^2}{\sigma^2+2 s^2}\nn\\
&\sim& \begin{cases}{1 - \smallfrac{s^2}{\sigma^2} \qquad\mbox{for $s^2\ll \sigma^2$;}}\\{ \half(1+\smallfrac{\sigma^2}{2s^2})\qquad \mbox{for $s^2\gg\sigma^2 $.}}\end{cases} 
\label{eq:modelRc}
\eea
The two ratios are shown in Fig.~\ref{fig:plot1}. When $s^2/\sigma^2$ is small, the effects from decorrelation are the strongest. When $s^2= \sigma^2$, as is the case for many experiments in NNPDF fits, $R=\smallfrac{5}{6}$ and $R_c = \frac{2}{3}$, so the decorrelation result is half way between the fully correlated and conservative cases.


\section{Predictions from one-parameter fits}
\label{sec:p2}

\section{Correlated MHOUs in PDF fits}
\label{sec:p3}

\section{Numerical results}
\label{sec:p4}

\section{Summary}
\label{sec:p5}
