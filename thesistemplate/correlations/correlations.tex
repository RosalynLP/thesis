\chapter{Making predictions using PDFs with theoretical uncertainties}

Earlier in this thesis we have discussed the importance of theoretical uncertainties, and how to include them in PDFs. We have also produced PDFs including MHOUs (see Chapter~\ref{chapter:mhous}) and deuteron and nuclear uncertainties (see Chapter~\ref{chapter:nuclear}), which are freely available. In the future, PDFs with theory uncertainties will become the norm, and will be used widely to make theoretical predictions for observables by convoluting them with parton-level hard cross sections. 

When making predictions for hadronic observables there are two sources of uncertainty: the hard cross section and the PDF. Typically, MHOUs for the former are estimated using scale variation, and these are added in quadrature to the latter. When the PDFs themselves also include MHOUs, we can think of the two scale evolutions considered being $Q_0 \to Q_{dat}$ and $Q_0 \to Q_{pred}$, where $Q_0$ is the PDF parametrisation scale, $Q_{dat}$ is the scale of data in the PDF and $Q_{pred}$ is the scale the prediction is made at. Then each of these sources has both a contribution from renormalisation scale variation and from factorisation scale variation.  The PDFs themselves contain a wealth of data from various processes, so when making a prediction for a process which is included in the PDFs there will invariably be correlations between the renormalisation scale variation in the PDF and that in the hard cross section. Even if the process is a new one, for example Higgs production, correlations due to factorisation scale variation will always be present. Simply combining the two sources of uncertainty in quadrature will miss these correlations and lead to an inflation in overall uncertainties. 

This issue was explored in detail in \cite{Harland-Lang:2018bxd}, noting that PDFs are a tool to express one observable in terms of others. This can be realised exactly in the simple case of fully correlated factorisation scale variation for non-singlet structure functions. Here the PDF can be eliminated entirely, and it is manifest that there is only one independent scale ($Q_{dat} \to Q_{pred}$), rather than two, with the MHOUs cancelling to a large degree. If MHOUs were used in both the PDF and the hard cross section in this case, it would amount to ``double counting". It was also shown that correlations existed for renormalisation scale variation, albeit less strongly.

When using PDFs in predictions we include both sources of scale variation in an uncorrelated way, and so miss the MHOU cancellation and corresponding reduction in uncertainties. As noted in \cite{MHOUbig}, this is a consequence of PDFs being universal. We cannot reconstruct the full data and MHOUs from the PDFs alone as information is lost in the fitting process; one set of PDFs could arise from many different data possibilities. However, if we want PDFs to be useful in a wide range of predictions, universality is required and so we must live with this loss of correlation.

Fortunately, the increase in PDF uncertainties due to MHOUs is small, and the effect is mostly realised in changes to the central value as the fit is rebalanced by changes to the weighting of different data. In indicative cases explored in Chapter 7 of \cite{MHOUbig}, it was seen that the PDF uncertainty is consequently much smaller than the MHOU on the hard cross section. When combining these two in quadrature, the effect of missing correlations will therefore likely be small, and so the overestimate of uncertainty will be small. It was argued that a small overestimate of uncertainty is better than neglecting MHOUs altogether. However, accounting for these correlations is desirable in order to keep uncertainties as low as possible in the current high precision era.

In this chapter we investigate correlations between PDF MHOUs and hard cross section MHOUs when making predictions. Although we focus on MHOUs, the analysis extends naturally to all sources of theory uncertainty. We develop a method for algebraically determining these correlations, and show how to include them when making a prediction. This is a complicated problem so we proceed incrementally:

\begin{itemize}
\item In Sec.~\ref{sec:p1} we show how a theoretical uncertainty can be reformulated in terms of a nuisance parameter, which holds the key to the propagation of uncertainties. We consider as a starting point an exact fit, showing that the experimental data partially decorrelate the theory uncertainties in the fit and those in the prediction.
\item In Sec.~\ref{sec:p2} we move on to a model where the data are fitted using just one parameter. Here the correlations lead to a shift in the theoretical predictions, but the fitting of the data leads to loss of information and consequently further decorrelation.
\item In Sec.~\ref{sec:p3} we extend this analysis to a multi-parameter fit with multiple theory uncertainties, and then to a PDF fit, where the PDFs are continuous functions with a functional uncertainty. We show that this functional uncertainty leads to a further decorrelation in the MHOUs. 
\item In Sec.~\ref{sec:p4} we present numerical results comparing this procedure with the naive approach of adding the uncorrelated contributions in quadrature, in the context of the NLO global fit with MHOUs discussed in Chapter \ref{chapter:mhous}. We make predictions including MHOUs for repetitions of all the experiments already in the fit (so-called ``autopredictions"). We then investigate the scenario of a prediction for a process already in the fit (top production), and for a new process (Higgs production), and show that including these correlations leads to a shift in the central value of the prediction that is well within the total uncertainty, and that the decorrelation of the MHOUs is almost complete. We see that in practice adding the two contributions in quadrature and keeping them uncorrelated (the ``conservative" prescription of Chapter \ref{chapter:mhous}) is remarkably accurate. Having said that, the methodology for including full correlations is now available. Finally, we suggest a novel way of improving theoretical predictions, using ths correlated shift.
\item In Sec.~\ref{sec:p5} we provide a summary.
\end{itemize}

\section{Predictions with correlated theory uncertainties}
\label{sec:p1}

We saw in Chapter \ref{chapter:thuncs} that to include theory uncertainties in a fit all you need to do is add a theory covariance matrix, $S_{ij}$, to the experimental covariance matrix $C_{ij}$. Recall that $i,j = 1, \dots, N_{dat}$ run over data points. The only assumptions underlying this result are that all uncertainties are Gaussian, and that the theory uncertainties are independent of the experimental data. Since Gaussian experimental uncertainties are already assumed in NNPDF's framework, these assumptions are very reasonable. We can express the result as the conditional probability
\be
\label{eqn:ptd}
P(T|D) \propto \exp \bigg( -\frac{1}{2} (T-D)^T(C+S)^{-1}(T-D) \bigg).
\ee
Recall that both $C$ and $S$ are real and symmetric, that $C$ is positive definite and that $S$ is positive semidefinite and will generally possess many zero eigenvalues. In a fit we determine $T$ from $D$ by maximising $P(T|D)$, which amounts to minimising
\be
\label{eqn:chi2}
\chi^2 =  (T-D)^T(C+S)^{-1}(T-D)
\ee
with respect to the free parameters which characterise the theory prediction.

In this section we start off by considering one single source of fully correlated theory uncertainty, so that
\be 
S = \beta \beta^T,
\ee
where $\beta$ are real and non-zero. 

\subsection{Nuisance parameters}
\label{subsec:nuis}
We can model the theory uncertainty as a fully correlated shift in the theory prediction:
\be 
T \to T + \lambda \beta,
\ee 
where $\lambda$ is a nuisance parameter characterising the scale of the shift. We will now show that this will lead us to Eqn.~\ref{eqn:ptd}. Firstly, assuming Gaussian experimental uncertainties, we can write
\be 
\label{eqn:nuisptd}
P(T|D\lambda) \propto \exp \bigg( -\frac{1}{2} (T + \lambda \beta -D)^T C^{-1}  (T + \lambda \beta -D) \bigg).
\ee
Using Bayes' Theorem, 
\be 
\label{eqn:bayes}
P(T|D\lambda) P(\lambda |D) = P(\lambda | TD)P(T|D).
\ee
We want to find $P(T|D)$, so we need an expression for the prior for $\lambda$, $P(\lambda |D) = P(\lambda)$, where we assume that the theory uncertainty is independent of the experimental data. We choose a unit-width Gaussian centred on zero, 
\be 
\label{eqn:lambdaprior}
P(\lambda) \propto \exp \bigg(-\frac{1}{2} \lambda^2 \bigg).
\ee
Marginalising over $\lambda$, Eqn.~\ref{eqn:nuisptd} becomes
\be 
\label{eqn:ptd2}
P(T|D) \propto \int d\lambda \exp \bigg( -\frac{1}{2} \bigg[ (T + \lambda \beta -D)^T C^{-1} (T + \lambda \beta -D) + \lambda^2 \bigg] \bigg).
\ee
We can evaluate the term in $[ \cdot ]$ by remembering $S= \beta \beta^T$, introducing the variable
\be 
\label{eqn:z}
Z \equiv (1 + \beta^T C^{-1} \beta)^{-1},
\ee
and completing the square:
\be 
\begin{split}
[ \cdot ] &= (T-D)^T C^{-1}(T-D) + (T-D)^T C^{-1} \lambda \beta + \lambda \beta^T C^{-1} (T-D) \\ &+ \lambda \beta^T C^{-1} \lambda \beta + \lambda^2 \\
& = (T-D)^T C^{-1}(T-D) + (T-D)^T C^{-1} \lambda \beta + \lambda \beta^T C^{-1} (T-D) + \lambda^2 Z^{-1} \\
&=  Z^{-1}(\lambda + Z \beta^T C^{-1} (T-D))^2 \\ &- Z(\beta^T C^{-1} (T-D))^2 + (T-D)^T C^{-1}(T-D) \\
&= Z^{-1}(\lambda + Z \beta^T C^{-1} (T-D))^2 \\ &+ (T-D)^T (C^{-1} - Z C^{-1} S C^{-1})(T-D).
\end{split}
\ee
Finally, we can use the Sherman-Morrison formula and Eqn.~\ref{eqn:chi2} to write this as
\be 
\begin{split}
[ \cdot ] &= Z^{-1}(\lambda + Z \beta^T C^{-1} (T-D))^2 + (T-D)^T (C + S)^{-1}(T-D) \\
&= Z^{-1}(\lambda + Z \beta^T C^{-1} (T-D))^2 + \chi^2 \\
&\equiv Z^{-1}(\lambda - \bar{\lambda}) + \chi^2,
\end{split}
\ee
where we have defined 
\be 
\label{eqn:lambdabardef}
\bar{\lambda} = Z \beta^T C^{-1} (D-T). 
\ee

Plugging this back into Eqn.~\ref{eqn:ptd2}, we get
\be  
\begin{split}
\label{eq:margresult}
P(T|D) &\propto \int d\lambda \ e^{-\frac{1}{2} \chi^2} \exp \bigg( -\frac{1}{2} Z^{-1} (\lambda - \bar{\lambda})^2 \bigg) \\
&\propto e^{-\frac{1}{2} \chi^2},
\end{split}
\ee
which is Eqn.~\ref{eqn:ptd}. The advantage of this approach is that we can also get the posterior distribution for $\lambda$ (after fitting using $D$ and $T$), by using Bayes' Theorem (Eqn.~\ref{eqn:bayes}):
\be 
\begin{split}
\label{eqn:lambdaposterior}
P(\lambda |TD) &= \frac{P(T|D\lambda) P(\lambda)}{P(T|D)} \\
&= \exp \bigg( -\frac{1}{2} \bigg[ (T+ \lambda \beta -D)^T C^{-1}(T+ \lambda \beta -D) + \lambda^2 - \chi^2 \bigg] \bigg) \\
& = \exp \bigg( -\frac{1}{2} Z^{-1}(\lambda - \bar{\lambda})\bigg),
\end{split}
\ee
where we recognised the similarity between the exponent here and in Eqn.~\ref{eqn:ptd2}. So the effect of the fit is to shift the centre of the distribution from 0 $\to \bar{\lambda}$, and the width from 1 $\to Z$. Note that from the definition of $Z$ (Eqn.~\ref{eqn:z}), 
\be 
0 \le Z \le 1,
\ee
so the theory uncertainty is always reduced when information on $D$ is added. Fig.~\ref{fig:lambdadistribs} gives a sketch of this effect.  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[H]
  \begin{center}
      \includegraphics[width=0.5\textwidth]{correlations/plots/lambdapriorpost.png}
    \caption{Sketch of the prior (Eqn.~\ref{eqn:lambdaprior}) and posterior (Eqn.~\ref{eqn:lambdaposterior}) distributions for $\lambda$. Adding information shifts the distribution and reduces the width. \label{fig:lambdadistribs}}
    
  \end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Decorrelation}
We will now test out this formalism in the case of a ``perfect fit". This is one where the theory inputs to the fit are flexible enough to fit the data exactly. It is not a especially realistic situation in the context of PDF fits, but will serve as a useful foundation on which to model more complex situations. 

In this perfect fit $P(T|D)$ (Eqn.~\ref{eqn:ptd2}) is maximised when $T=D$, which corresponds to $\chi^2 = 0$. The data and theory are therefore fully correlated, and the expectation value, $E[\cdot]$, and covariance, $\Cov[\cdot]$, of the theory are
\be
\label{eq:perfectfit}
E[T] = D,\qquad \Cov[T] = \Cov[D] = C+S.
\ee
Looking back at the definition of $\bar{\lambda}$, we can see that after the fit $\bar{\lambda}=0$, and so Eqn.~\ref{eqn:lambdaposterior} is independent of $T$. The posterior for $\lambda$ has 
\be 
\label{eq:meanvarlam}
E[\lambda] =0,\qquad \Var[\lambda] = Z.
\ee
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[H]
  \begin{center}
      \includegraphics[width=0.5\textwidth]{correlations/plots/lambdaperfectfit.png}
    \caption{Sketch of the prior (Eqn.~\ref{eqn:lambdaprior}) and posterior (Eqn.~\ref{eqn:lambdaposterior}) distributions for $\lambda$ in the case of a perfect fit. The additional information from $D$ reduces the uncertainties but there is no shift as the theory fit the data exactly. \label{fig:lambdaperfect}}
   
  \end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Fig.~\ref{fig:lambdaperfect} shows a sketch of the effect on the distribution of $\lambda$. What happens if we fit $T$ to $D$ and then use the results of this fit to make predictions for $T$? We can call this an ``autoprediction", because we are making predictions about the theory included in the fit itself. In effect this is like making theory predictions for another set of experimental results where the settings tie up exactly with those already included in the fit. In this scenario, the uncertainty is reduced as the experimental data have added information. Remembering that we can write $T(\lambda) = T + \lambda \beta$, we find
\be
\label{eq:perfectcovT}
\begin{split}
&E[T(\lambda)] = E[T]=D \\ &{\Cov}[T(\lambda)] 
= {\Cov}[T] + \Var[\lambda] \beta\beta^T = C+S+ZS.
\end{split}
\ee
There are three contributions to the covariance: the experimental uncertainty in the data, the theory uncertainty in the fit, and the theory uncertainty in the prediction. Note that the theory uncertainty in the prediction is reduced by a factor $Z={\rm Var}[\lambda]$, because of the correlation between this and the theory uncertainty in the fit. Note that:
\begin{itemize}
\item ignoring correlations amounts to setting $Z=1$ and we get ${\Cov}[T(\lambda)] = C + 2S$. This is the ``conservative prescription" of Chapter.~\ref{chapter:mhous}.
\item adopting the approach of \cite{Harland-Lang:2018bxd} and assuming full correlations amounts to setting $Z=0$ and we get ${\Cov}[T(\lambda)] = C + S$, so the conservative prescription double counts with respect to this.
\end{itemize}
In reality, the situation is somewhere between these two; we see a partial decorrelation due to the presence of experimental uncertainties.

To get a better sense of the size of decorrelation, let's consider a simple model where the experimental covariance matrix is diagonal and the theory uncertainty is fully correlated. Writing $\beta = s e$, where $s$ is the size of correlated theory uncertainty and $e^Te=1$, we have
\be
\label{eq:modelCS}
C = \sigma^2 \mathbb{I},\qquad S = s^2 e e^T,
\ee
where $\sigma$ is the per-point uncertainty. Then we can evaluate:
\be
\begin{split}
\label{eq:modelCplusSinv}
(C+S)^{-1} &= \frac{1}{\sigma^2}\left(1-\frac{s^2}{\sigma^2+s^2}e e^T\right); \\
Z &= (1+s^2/\sigma^2)^{-1}.
\end{split}
\ee
so we see that the extent of decorrelation depends on the ratio $s^2/\sigma^2$:
\begin{itemize}
\item as $\sigma^2/s^2 \to 0$ we tend to the case of no experimental uncertainties, as considered in \cite{Harland-Lang:2018bxd}, and $Z \to 0$.
\item as $s^2/\sigma^2 \to 0$, $Z \to 1$, which is the conservative prescription.
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[h]
  \begin{center}
      \includegraphics[width=0.5\textwidth]{correlations/plots/plot1.pdf}
    \caption{ For a perfect fit within the model Eqn.~\ref{eq:modelCS}.  Comparing the ratios to the conervative prescription of the decorrelated ($R$, Eqn.~\ref{eq:modelR}) and correlated ($R_c$, Eqn.~\ref{eq:modelRc}) autoprediction covariance matrices.  \label{fig:plot1}}
  \end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Now consider the covariance matrix of autopredictions. In the conservative case this is $C + 2S$, which in the direction of $e$ is $\sigma^2 + 2s^2$. In any direction orthogonal to $e$ it is just $\sigma^2$ because there are no theory uncertainties in these directions. We can calculate the ratio of the true covariance matrix to the conservative one within the model in Eqn.~\ref{eq:modelCS}. This ratio is
\bea
R = \frac{e^T(C+S+ZS)e}{e^T(C+2S)e} &=& \frac{\sigma^2+(1+Z)s^2}{\sigma^2+2s^2}\nn\\
&=& 1-\frac{s^4}{\sigma^4+3\sigma^2 s^2 + 2s^4}\nn\\
&\sim&  \begin{cases}{1 -\smallfrac{ s^4}{\sigma^4} \qquad\mbox{for $s^2\ll\sigma^2 $;}}\\{ \half(1+\smallfrac{3\sigma^2}{2s^2})\qquad \mbox{for $s^2 \gg \sigma^2$.}}\end{cases} 
\label{eq:modelR}
\eea
We can see that
\begin{itemize}
\item when experimental uncertainties dominate then the theory uncertainties are almost entirely decorrelated. In this case the conservative prescription is almost exact; only a slight overestimate.
\item when theory uncertainties dominate then there is very little decorrelation, and the conservative prescription double counts the theory uncertainties.
\end{itemize}
When we're between these extremes, we can look at the difference between this ratio and that for the fully correlated case, which is
\bea
R_c = \frac{e^T(C+S)e}{e^T(C+2S)e} &=& 1 -\frac{s^2}{\sigma^2+2 s^2}\nn\\
&\sim& \begin{cases}{1 - \smallfrac{s^2}{\sigma^2} \qquad\mbox{for $s^2\ll \sigma^2$;}}\\{ \half(1+\smallfrac{\sigma^2}{2s^2})\qquad \mbox{for $s^2\gg\sigma^2 $.}}\end{cases} 
\label{eq:modelRc}
\eea
The two ratios are shown in Fig.~\ref{fig:plot1}. When $s^2/\sigma^2$ is small, the effects from decorrelation are the strongest. When $s^2= \sigma^2$, as is the case for many experiments in NNPDF fits, $R=\smallfrac{5}{6}$ and $R_c = \frac{2}{3}$, so the decorrelation result is half way between the fully correlated and conservative cases.


\section{Predictions from one-parameter fits}
\label{sec:p2}
In the previous section we considered a perfect fit, where after fitting $T=D$. Now we consider the slightly more realistic scenario, where the fit is not perfect, but the theory predictions, $T(\theta)$, only depend on a single parameter, $\theta$. This is a useful model because it captures the essence of the fitting problem but it is sufficiently simple that we will be able to solve it exactly. The $\chi^2$ (Eqn.~\ref{eqn:chi2}) will be minimised for some $\theta = \theta_0$, with variance $\Var [\theta]$. Once $\theta_0$ has been determined we can then make some predictions, $\Ttil(\theta_0)$, where the tilde denotes they are predictions for theories separate from the fit inputs. These predictions will have uncertainties proportional to $\Var [\theta]$. 

We have assumed that the uncertainties are Gaussian, and so they are differentiable. This means we can linearise $T(\theta)$ about $T(\theta_0) \equiv T_0$:
\be
\label{eq:Tlin}
T(\theta) = T_0 + (\theta-\theta_0)\Tdot_0 + \dots .
\ee
We want to determine the uncertainty in the fitted $\theta$, so we need to propagate the uncertainties in the data, $D$, and the theory, $T(\theta)$, into $\theta$. We can do this using the standard NNPDF approach (see Chapter ~\ref{chapter:background}) of generating pseudodata replicas, $D^{(r)}$, which are Gaussianly distributed about the actual data, $D$, with covariance $C+S$. More explicitly, we can define the average over replicas for any function, $F$, of the replicas as
\be
\label{eq:repav}
\langle F(D^{(r)})\rangle =  \smallfrac{1}{N_{\rm rep}}\sum_{r=1}^{N_{\rm rep}}F(D^{(r)}).
\ee
Then the replicas will satisfy
\be 
\label{eq:repavD}
\langle D^{(r)}\rangle \equiv D^{(0)}, \qquad \langle (D^{(r)}-D^{(0)})(D^{(r)}-D^{(0)})^T\rangle = C+S.
\ee
In the limit of $N_{rep} \to \infty$, $D^{(0)} \to D$, and for finite replicas $D^{(0)}$ differs from $D$ by an amount which tends to zero like $N_{rep}^{-1/2}$.

The fit proceeds by fitting a parameter replica, $\theta^{(r)}$, for each pseudodata replica, $D^{(r)}$, by minimising
\be
\label{eq:chi2rep}
\chi_r^2[\theta] = (T(\theta)-D^{(r)})^T(C+S)^{-1}(T(\theta)-D^{(r)}),
\ee
with respect to $\theta$. Using Eqn.~\ref{eq:Tlin}, this leads to 
 \be
\label{eq:arep}
\theta^{(r)} - \theta_0 = \frac{\Tdot_0^T(C+S)^{-1}(D^{(r)}-T_0)}{\Tdot_0^T(C+S)^{-1}\Tdot_0}.
\ee
Now $\theta_0 = \langle \theta^{(r)} \rangle$, so using the replica averages in Eqn.~\ref{eq:repavD} we find
\be
\label{eq:consistency}
\Tdot_0^T(C+S)^{-1}(D-T_0)=0,
\ee
so we can rewrite Eqn.~\ref{eq:arep} as
\be
\label{eq:arep2}
\theta^{(r)} - \theta_0 = \frac{\Tdot_0^T(C+S)^{-1}(D^{(r)}-D)}{\Tdot_0^T(C+S)^{-1}\Tdot_0}.
\ee
Using the fact that $C$ and $S$ are symmetric, 
\bea
\Var[\theta] &=& \langle(\theta^{(r)}-\theta_0)^2\rangle\nn\\
 &=& \frac{\Tdot_0^T(C+S)^{-1}\langle(D^{(r)}-D)(D^{(r)}-D)^T\rangle (C+S)^{-1}\Tdot_0}{(\Tdot_0^T(C+S)^{-1}\Tdot_0)^2}\nn\\
&=& (\Tdot_0^T(C+S)^{-1}\Tdot_0)^{-1}.
\label{eq:vara}
\eea
Note that:
\begin{itemize}
\item data points with a large dependence on $\theta$ have large $\Tdot_0$ and contribute more.
\item directions with large uncertainty, $(C+S)$, contribute less.
\end{itemize}
Now we have the uncertainty in the fitted parameter, $\theta$, we can find the fitting uncertainty. This is the covariance of $T(\theta)$ due to the experimental and theoretical uncertainties from fitting $\theta$. We will call this covariance matrix $X$. Using the fact that $E[T] = \langle T(\theta^{(r)})\rangle = T(\theta_0) = T_0$ and writing $T^{(r)} = T(\theta^{(r)})$,
\bea
X\equiv\Cov[T(\theta)] &=& \langle(T^{(r)}-T_0)(T^{(r)}-T_0)^T\rangle\label{eq:Xdef}\\
&=& \Tdot_0\langle(\theta^{(r)}-\theta_0)^2\rangle\Tdot_0^T\\
&=& \Tdot_0(\Tdot_0^T(C+S)^{-1}\Tdot_0)^{-1}\Tdot_0^T\label{eq:Xdef2}\\
&=& n(n^T(C+S)^{-1}n)^{-1}n^T,
\label{eq:Xdef3}
\eea
where in the last line we define $\Tdot_0 \equiv |\Tdot_0|n$, i.e. $n$ is a unit vector in the direction of $\Tdot_0$. We can see that $X$ depends only on the direction ($n$) of $\Tdot_0$, not its magnitude. Note that $X$ is singular and also that
\be
\label{eq:XsqeqX}
X = X(C+S)^{-1}X,
\ee
which will be useful later. Using Eqn.~\ref{eq:arep2} in Eqn.~\ref{eq:Tlin}, we can see that
\be
T^{(r)}-T_0 = X(C+S)^{-1}(D^{(r)}-D^{(0)}),
\label{eq:projection}
\ee
so $X(C+S)^{-1}$ projects the data replicas onto the theory replicas.

Now let's revisit the model for covariance matrices, Eqn.~\ref{eq:modelCS}. If we define the angle between the experimental uncertainties and the $\theta$ variation by $\cos \phi = n^T e$, we find that
\bea
\label{eq:denom}
n^T(C+S)^{-1}n &=& \frac{\sigma^2+s^2\sin^2\phi}{\sigma^2(\sigma^2+s^2)},\\
n^TXn &=& \frac{\sigma^2(\sigma^2+s^2)}{(\sigma^2+s^2\sin^2\phi)}.
\eea
Note that using any vector other than $n$ here gives 0. We can see that the effects from the theory uncertainty ($s$) depend on its degree of alignment with $n$, the direction of the parameter dependence. 
\begin{itemize}
\item For complete alignment, $\phi=0$ and the variance of $T$ in this direction is $(\sigma^2 + s^2)$.
\item When they are orthogonal, $\phi= \smallfrac{\pi}{2}$ and the variance is $\sigma^2$, so the theory uncertainty doesn't factor into the fitting.
\end{itemize}

\subsection{Autopredictions in a single parameter fit}
Now let's consider making autopredictions in this single parameter fit,
\be
\label{eq:autopred}
T(\theta,\lambda)=T(\theta)+\lambda\beta .
\ee
When fitting, we:
\begin{enumerate}
\item Maximise $P(T(\theta)|D)$ replica by replica to get $\{ \theta^{(r)} \}$.
\item Maximise $P(\lambda |T(\theta) D)$ replica by replica, using the values $\{ \theta^{(r)} \}$ from 1., to get  $\{ \lambda^{(r)} \}$.
\item Average over replicas to obtain the fitted results.
\end{enumerate}
For example, from Eqn.~\ref{eqn:lambdabardef} we can write
\be
\label{eq:lambdabarz}
E[\lambda] = \langle \bar{\lambda}(\theta^{(r)}) \rangle =  \overline\lambda(\theta_0) = \beta^T(C+S)^{-1}(D^{(0)}-T_0).
\ee
Now that in general $D \neq T_0$, the nuisance parameters can have non-zero expectation values, and this will contribute to the expectation value of the autopredictions:
\be
\label{eq:ET}
E[T(\theta,\lambda)] = \langle T(\theta^{(r)},\overline\lambda(\theta^{(r)})\rangle =  
T(\theta_0,\overline\lambda(\theta_0)) = T(\theta_0)+\overline\lambda_0\beta.
\ee
So the data give information on the theory uncertainty by inducing shifts in the autopredictions:
\be
\label{eq:shift}
\delta T(\theta_0) = \beta\beta^T(C+S)^{-1}(D^{(0)}-T(\theta_0)) = S(C+S)^{-1}(D^{(0)}-T(\theta_0)).
\ee
Recall that $n^T(C+S)^{-1}(D^{(0)}-T(\theta_0)) =0$ from Eqn.~\ref{eq:consistency}, so we only get non-zero shifts when $n$ and $e$ (the data and the theory) point in different directions. When they are parallel ($\theta =0$), the theory uncertainty is just absorbed into the fit.

Now let's look at the change in the uncertainty of the autopredictions relative to the original predictions. The variance of $\lambda$ has two independent contributions in this scenario:
\begin{enumerate}
\item the width of the posterior, $P(\lambda | T(\theta) D)$;
\item the fluctuation of $\lambda (\theta)$ due to changing $\theta$ over replicas.
\end{enumerate}
So we can write
\be
\label{eq:varlamdef}
\Var[\lambda] = E[(\lambda-\overline\lambda_0)^2] = E[(\lambda-\overline\lambda(\theta^{(r)})^2]+ \langle(\overline\lambda(\theta^{(r)})-\overline\lambda_0)^2\rangle.
\ee
It's easiest to first address these two contributions separately. The first term is analogous to that in Sec.~\ref{sec:p1}, and we can use the Sherman Morrison formula again to rewrite it:
\be
\begin{split}
\label{eq:Zdef2}
E[(\lambda-\overline\lambda(\theta^{(r)})^2] = Z &= (1 + \beta^T C^{-1}  \beta)^{-1} \\ &= 1-\beta^T(C+S)^{-1}\beta,
\end{split}
\ee
As for the second term, we can use the definitions of $\bar{\lambda}$ and $\bar{\lambda}_0$, and then Eqn.~\ref{eq:projection} to write
\bea
 \overline\lambda(\theta^{(r)})-\overline\lambda_0 &=& \beta^T(C+S)^{-1}(D^{(r)}-D^{(0)}-(T^{(r)}-T_0))\nn\\
&=& \beta^T(C+S)^{-1}(D^{(r)}-D^{(0)}-(\theta^{(r)}-\theta_0)\Tdot_0)\nn\\
&=& \beta^T(C+S)^{-1}(1-X(C+S)^{-1})(D^{(r)}-D^{(0)}).\label{eq:lamalgebra}
\eea
So we can then get
\bea
 \langle(\overline\lambda(\theta^{(r)})-\overline\lambda_0)^2\rangle &=&
\beta^T(C+S)^{-1}(1-X(C+S)^{-1})\langle(D^{(r)}-D^{(0)})(D^{(r)}-D^{(0)})^T\rangle\nn\\
&&\qquad\qquad \times(1-(C+S)^{-1}X)(C+S)^{-1}\beta\nn\\.
\eea
Then using Eqn.~\ref{eq:repavD} followed by Eqn.~\ref{eq:XsqeqX}:
\bea
 \langle(\overline\lambda(\theta^{(r)})-\overline\lambda_0)^2\rangle &=&\beta^T((C+S)^{-1}-2(C+S)^{-1}X(C+S)^{-1}\nn\\
&&\qquad +(C+S)^{-1}X(C+S)^{-1}X(C+S)^{-1})\beta
\label{eq:varlambar1}\\
&=&\beta^T(C+S)^{-1}\beta-\beta^T(C+S)^{-1}X(C+S)^{-1}\beta
\label{eq:varlambar2}
\eea
So overall 
\be
\label{eq:Zbardef}
\Var[\lambda] = 1 -\beta^T(C+S)^{-1}X(C+S)^{-1}\beta\equiv \Zbar.
\ee
We can see that $\bar{Z} \leq 1$ because $(C+S)^{-1}X(C+S)^{-1}$ is positive semidefinite, and noting that $\bar{Z} = Z + \langle(\overline\lambda(\theta^{(r)})-\overline\lambda_0)^2\rangle$, it is clear that $\bar{Z} \geq Z$. When $n=e$, so that the $\theta$ variation and the theory uncertainty are aligned, we get $\bar{Z} = Z$. So in summary
\be
\label{zbarbounds}
0<Z\leq\Zbar\leq 1.
\ee
In other words, there is more decorrelation in this constrained fit than in the perfect fit of Sec.~\ref{sec:p1}, because the fit uncertainty causes additional decorrelation. This is the second mechanism for decorrelation we have uncovered, on top of the decorrelation due to experimental uncertainties observed in Sec.~\ref{sec:p1}.

Finally we can use the model for uncertainties (Eqn.~\ref{eq:modelCS}) to show that
\be
\label{eq:Zbarmod}
\Zbar = \frac{\sigma^2 + s^2 \sin^2\phi}{\sigma^2+s^2}.
\ee
So, comparing this with the expression for $Z$ (Eqn.~\ref{eq:modelCplusSinv}), $\bar{Z} = Z$ when $\phi =0$, as expected, and $\bar{Z}=1$ when $\phi = \smallfrac{\pi}{2}$; here the data have no effect on the uncertainty, so we have total decorrelation.

Let's also calculate the covariance of autopredictions in the model. This is made up of two contributions:
\begin{enumerate}
\item the variation about $\bar{\lambda}$;
\item the fluctuations of the individual replicas in the fit.
\end{enumerate}
\bea
{\Cov}[T(\theta,\lambda)] &=& E[(\lambda-\lambdabar(\theta))^2]\beta\beta^T 
 + \Cov[T(\theta,\lambdabar(\theta)]\label{eq:covTsumx}\nn\\
&=& ZS 
 + \langle(T(\theta^{(r)},\lambdabar(\theta^{(r)})) -T(\theta_0,\lambdabar_0))^2 \rangle  \label{eq:covTsum}
\eea
Let's first calculate
\bea
T(\theta^{(r)},\lambdabar(\theta^{(r)}))-T(\theta_0,\lambdabar_0) &=& T(\theta^{(r)}) - T(\theta_0) + (\bar{\lambda}(\theta^{(r)}) - \bar{\lambda_0}) \beta \\
&=& X(C+S)^{-1}(D^{(r)}-D^{(0)}) \\ &&\qquad + \beta \beta^T (C+S)^{-1}(1-X(C+S)^{-1}) \qquad \qquad \\ && \qquad \times (D^{(r)}-D^{(0)}) \\
&=& [ X(C+S)^{-1} \\ && \qquad + S(C+S)^{-1}(1-X(C+S)^{-1}) ] \qquad \\ && \qquad \times  (D^{(r)}-D^{(0)}).
\eea
Now consider right multiplying the term in $[ \cdot ]$ by $\mathbb{I} = (C+S)(C+S)^{-1}$, and it can be shown that
\be
T(\theta^{(r)},\lambdabar(\theta^{(r)}))-T(\theta_0,\lambdabar_0) = (S+C(C+S)^{-1}X)(C+S)^{-1}(D^{(r)}-D^{(0)}).\label{eq:Tarep}
\ee
So, using Eqn.~\ref{eq:XsqeqX} to simplify the quadratic in $X$, the second term is
\bea
{\Cov}[T(\theta,\lambdabar)]&=&\langle(T(\theta^{(r)},\lambdabar(\theta^{(r)}))-T(\theta_0,\lambdabar_0))(T(\theta^{(r)},\lambdabar(\theta^{(r)}))-T(\theta_0,\lambdabar_0))^T\rangle\nn\\ 
&=& (S+C(C+S)^{-1}X)(C+S)^{-1}(S+X(C+S)^{-1}C)\nn\\ \qquad
&=&  S(C+S)^{-1}S + X - S(C+S)^{-1}X(C+S)^{-1}S. \qquad \qquad \label{eq:covTrlamr}
\eea
So the total covariance of autopredictions is
\bea
{\Cov}[T(\theta,\lambda)] &=& (S - S(C+S)^{-1}S) + (S(C+S)^{-1}S +X - S(C+S)^{-1}X(C+S)^{-1}S)\nn\\
&=& X + S - S(C+S)^{-1}X(C+S)^{-1}S \\
&=& X + \bar{Z} S. \label{eq:covTfit2}
\eea
This is the same as if we had assumed that $T(\theta)$ and $\lambda$ are uncorrelated, i.e. that
\be
{\Cov}[T(\theta,\lambda)] = {\Cov}[T] + \Var[\lambda] \beta\beta^T 
= X+\Zbar S.\label{eq:covTfit}
\ee
So we see that the cross correlations between $T(\theta)$ and $\lambda$ have cancelled.

We can use the model again to calculate the ratio of this covariance to the one given by the conservative prescription:
\bea
\Rbar &=& \frac{e^T(X+\Zbar S)e}{e^T(X+S)e}\nn\\
&=& 1 - \frac{s^4\cos^2\phi}{\sigma^2+ s^2}\frac{\sigma^2+s^2\sin^2\phi}{\sigma^2(\sigma^2+s^2)\cos^2\phi+s^2(\sigma^2+s^2\sin^2\phi)}\nn\\
&\sim&  \begin{cases}{1 -\smallfrac{ s^4}{\sigma^4} \qquad\qquad\mbox{for $s^2 \ll \sigma^2$;}}\\{ \sin^2\phi +\smallfrac{\sigma^2{\rm cot}^2\phi}{s^2}\qquad \mbox{for $s^2 \gg \sigma^2$.}}\end{cases} 
\label{eq:modelRbar}
\eea
\begin{itemize}
\item When the experimental uncertainties dominate, $\sigma^2 \gg s^2$  and there is almost total decorrelation, as before. The conservative prescription is a very good approximation.
\item When the theory uncertainties dominate, $s^2 \gg \sigma^2$, and the extent of decorrelation is $ \propto \sin^2 \phi$. If $n$ and $e$ are closely aligned this is small, but if they're orthogonal then fitting the data has no effect on the theory uncertainty, so we have total decorrelation. In that case the assumption of full correlation is a big underestimate. 
\end{itemize}
The ratio to the full correlation case is
\bea
\Rbar_c &=& \frac{e^TXe}{e^T(X+S)e}\nn\\
&\sim& \begin{cases}{1 - \smallfrac{s^2}{\sigma^2\cos^2\phi} \qquad\mbox{for $s^2 \ll \sigma^2$;}}\\{ \smallfrac{\sigma^2{\rm cot}^2\phi}{s^2}\qquad \mbox{for $s^2 \gg \sigma^2$.}}\end{cases} 
\label{eq:modelRbarc}
\eea
Fig.~\ref{fig:plot2} shows a comparison between these two. When $\phi$ is small, the limit of $\smallfrac{s^2}{\sigma^2}$ is reached much more slowly because it requires $s^2 \gg \sigma^2 \cot^2 \phi$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[h]
  \begin{center}
      \includegraphics[width=0.5\textwidth]{correlations/plots/plot2.pdf}
    \caption{ For single parameter fit within the model Eqn.~\ref{eq:modelCS}.  Comparing the ratios to the conervative prescription of the decorrelated ($\Rbar$, Eqn.~\ref{eq:modelRbar}) and correlated ($\Rbar_c$, Eqn.~\ref{eq:modelRbarc}) autoprediction covariance matrices.  \label{fig:plot2}}
  \end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Correlated predictions}
Let's think about making predictions for other observables not in the fit. We denote these $\Ttil_I(\theta)$, $I=1,\ldots , \widetilde{N}$. They depend on the same $\theta$ parameter as the fitted predictions, $T_i(\theta)$, $i=1, \ldots , N$. There are two sources of uncertainty in $\Ttil_I(\theta)$:
\begin{enumerate}
\item that coming from the fit, i.e. the uncertainty in $\theta$ due to the experimental uncertainty in $D_i$ and the theory uncertainty in $T_i(\theta)$.
\item the theory uncertainty in $\Ttil_I(\theta)$ themselves.
\end{enumerate}

Source 1. is expressed via Eqn.~\ref{eq:vara}. We can write
\be
\label{eq:Tlin2}
\Ttil(\theta) = \Ttil_0 + (\theta-\theta_0)\Ttildot_0,
\ee
and use the shorthand notation $\Ttil^{(r)}\equiv\Ttil(\theta^{(r)})$. Then the covariance of $\Ttil_I(\theta)$ due to the uncertainty in $\theta$ is just like we found before for the autopredictions:
\bea
\Xtil\equiv\Cov[\Ttil(\theta)] &=& \langle(\Ttil^{(r)}-\Ttil_0)(\Ttil^{(r)}-\Ttil_0)^T\rangle\label{eq:Xtildef}\\
&=& \Ttildot_0\langle(\theta^{(r)}-\theta_0)^2\rangle\Ttildot_0^T
= \Ttildot_0(\Tdot_0^T(C+S)^{-1}\Tdot_0)^{-1}\Ttildot_0^T \qquad \qquad \label{eq:Xtildef2}.
%\\
%&=& \rtil\ntil(n^T(C+S)^{-1}n)^{-1}\ntil^T.
%\label{eq:Xtildef3}
\eea

Source 2. can be either correlated or uncorrelated with that in $T_i(\theta)$. Let's consider first the case where it is uncorrelated, for example if $\Ttil(\theta)$ are for a different type of observable to $T(\theta)$. Then we can associate with them a nuisance parameter, $\lambdatil$, which is uncorrelated with $\lambda$, and has the same prior as $\lambda$. This means we can write
\be
\Ttil(\theta,\lambdatil) =  \Ttil(\theta) + \lambdatil\betatil,\label{eq:uncor}
\ee
Here $\betatil_I$ give the size of the theory uncertainty in $\Ttil_I(\theta)$. We can proceed by finding the expectation value and covariance of the predictions:
\bea
E[\Ttil(\theta,\lambdatil)] &=& \Ttil(\theta_0),\label{eq:uncorpred}\\
\Cov[\Ttil(\theta,\lambdatil)]&=& \Cov[\Ttil(\theta)]+\Var[\lambdatil]\betatil\betatil^T 
= \Xtil + \Stil,\label{eq:uncorcov}
\eea
where 
\be
\Stil = \betatil\betatil^T,\label{eq:Stildef}
\ee
is the theory covariance matrix of predictions. What this shows us is that when the theory uncertainty is uncorrelated, we just add it in quadrature to the uncertainty due to $\theta$ in the fit. This matches the recommendations of the conservative prescription.

How about when $\Ttil(\theta)$ are fully correlated to $T(\theta)$? In this case $\lambdatil = \lambda$, and we already have the expectation value and covariance of $\lambda$ for the fit in Eqns.~\ref{eq:lambdabarz},\ref{eq:Zbardef}. So
\be
\label{eq:corpredmean}
E[\Ttil(\theta,\lambda)] = \Ttil(\theta_0) + \overline\lambda(\theta_0)\betatil,
\ee
and we get a shift in predictions analogous to the case of the autopredictions, i.e.
\bea
\label{eq:shiftpred}
\delta \Ttil(\theta_0) &=& \betatil\beta^T(C+S)^{-1}(D^{(0)}-T(\theta_0)) \\ &=& \Shat (C+S)^{-1}(D^{(0)}-T(\theta_0)).
\eea
Here 
\be
\Shat = \betatil\beta^T,\label{eq:Shatdef}
\ee
is the cross-correlated theory covariance matrix between $T(\theta)$ and $\Ttil(\theta)$. We can also define, in analogy to Eq.~(\ref{eq:Xdef}) and Eq.~(\ref{eq:Xtildef2}),
\be
\Xhat = \Ttildot_0(\Tdot_0^T(C+S)^{-1}\Tdot_0)^{-1}\Tdot_0^T\label{eq:Xhat},
\ee
and note the corresponding projective relations 
\be
\Xhat(C+S)^{-1}X=\Xhat,\qquad X(C+S)^{-1}\Xhat^T=\Xhat^T,\qquad \Xhat(C+S)^{-1}\Xhat^T=\Xtil \label{eq:XhatsqeqXtil} .
\ee
Following on from this, we can write
\be
\Ttil(\theta^{(r)})-\Ttil(\theta_0) = \Xhat (C+S)^{-1}(D^{(r)}-D^{(0)}),\label{eq:Ttilrep}
\ee
and so
\be
\Ttil(\theta^{(r)},\lambdabar(\theta^{(r)}))-\Ttil(\theta_0,\lambdabar_0) = (\Xhat+\Shat-\Shat(C+S)^{-1}X)(C+S)^{-1}(D^{(r)}-D^{(0)}),\label{eq:Ttilarep}
\ee
which leads us to the contribution from Source 2.:
\be
\Cov[\Ttil(\theta,\lambdabar(\theta)] =  \Shat (C+S)^{-1}\Shat^T + \Xtil - \Shat(C+S)^{-1}X(C+S)^{-1}\Shat^T.\label{eq:covTtilrlamr}
\ee

Then the full covariance of the predictions is the combination of Source 1. and Source 2. (using Eqn.~\ref{eq:Zdef2}):
\bea
{\Cov}[\Ttil(\theta,\lambda)] &=& E[(\lambda-\lambdabar(\theta))^2]\Stil 
 + \Cov[\Ttil(\theta,\lambdabar(\theta)]\label{eq:covTtilsumx}\\
&=& (1-\beta^T (C+S)^{-1} \beta)\Stil + \Shat (C+S)^{-1}\Shat^T + \Xtil \\ &&\qquad - \Shat(C+S)^{-1}X(C+S)^{-1}\Shat^T \\
&=& \Xtil + \Stil - \Shat(C+S)^{-1}X(C+S)^{-1}\Shat^T.\label{eq:covTtilfit2}\\
&=& \Xtil +\Zbar \Stil. \\
\eea

In summary, including correlations betwen the theory uncertainties in the fit and the predictions:
\begin{itemize}
\item shifts the central value of the prediction;
\item reduces the uncertainty of the prediction.
\end{itemize}
The fit gives information from $D$ about the theory uncertainties, leading to a more precise prediction. Note that all of the theory covariance matrices are positive semidefinite, and that
\be
\label{eq:possemdeftil}
0\leq \Shat^T(C+S)^{-1}\Shat \leq \Stil,
\ee
so ${\Cov}[\Ttil(\theta,\lambda)]$ is always reduced by the subtracted term in Eqn.~\ref{eq:covTtilfit2}, but will never be negative (which would be nonsensical in this context). 

We can understand a little more about the various theory covariance matrices by imagining we obtained some experimental measurements, $\widetilde{D}$, corresponding to predictions $\Ttil$. Then we could add these to the fit, and would get a new fitting theory covariance matrix with dimensions $(N+\widetilde{N})\times(N+\widetilde{N})$ and content
\be
\left(\begin{array}{cc}
S&\Shat\\
\Shat^T&\Stil\end{array}\right)
 =
\left(\begin{array}{cc}
\beta\beta^T&\beta\betatil^T\\
\betatil\beta^T&\betatil\betatil^T\end{array}\right).
\label{eq:covmatglobal}
\ee
Here the role of $\Shat$ and $\Shat^T$ as cross-correlation theory covariance matrices is clear. We can see that the theory uncertainties in the prediction are consistent with the theory uncertainties we would use when including observables in a fit, which makes sense in the comparison with the autopredictions case. 

\section{Correlated MHOUs in PDF fits}
\label{sec:p3}
In this section we add another layer of complexity to the model we are building up. Now the theory values, $T_i[f]$, depend on PDFs, $f$, which are determined in a global fit to the $N$ data points, $D_i$, with experimental covariance $C_{ij}$. The PDFs are then used to make $\widetilde{N}$ theory predictions, $\Ttil_I[f]$.

In a PDF fit there are many potential sources of theory uncertainty, but here we will consider MHOUs, computed with scale variation using a prescription from Chapter~\ref{chapter:mhous}. In this case $S_{ij}$ and $\Stil_{IJ}$ have many non-zero eigenvalues. We can describe them using $n$ nuisance parameters, $\lambda_\alpha$, $\alpha = 1, \dots , n$. Usually $n \ll N$, but we don't impose a limit on $n$ here. 

We will now find the expectation value and covariance of these nuisance parameters, and use those to find the shifts in predictions, and  the change in their uncertainties.

\subsection{Multiple nuisance parameters}
\label{subsec:p31}
Here we repeat the analysis of Sec.~\ref{subsec:nuis}, but for multiple nuisance parameters rather than just one. Each nuisance parameter is associated with a shift in theory value $T_i[f]\to T_i[f] + \lambda_\alpha\beta_{i,\alpha}[f]$, using summation notation for $\alpha$. Note that the $\beta_{i,\alpha}$ don't have to be mutually orthogonal. We pick a prior for each nuisance parameter the same as in Sec.~\ref{subsec:nuis}, i.e. 
\be
\label{eq:priorf}
P(\lambda|D)=P(\lambda) \propto \exp\big(-\half\lambda_\alpha\lambda_\alpha\big).
\ee
Once again we assume Gaussianity, and now instead of Eq.~(\ref{eq:PTDl}) we get
\be
\label{eq:PTDlf}
P(T|D\lambda)\propto \exp\big(-\half(T[f]+\lambda_\alpha\beta_\alpha-D)^TC^{-1}(T[f]+\lambda_\alpha\beta_\alpha-D)\big).
\ee
We can marginalise over $\lambda_\alpha$ to get 
\be
\label{eq:marginalise2}
P(T|D) \propto\int d^n\lambda\, \exp\left(-\half[(T[f]+\lambda_\alpha\beta_\alpha-D)^TC^{-1}(T[f]+\lambda_\beta\beta_\beta-D)+\delta_{\alpha\beta}\lambda_\alpha\lambda_\beta]\right)\, .
\ee
The next step is to complete the square in the exponent. After some work, defining 
\be
\label{eq:zdefmat}
Z_{\alpha\beta} = (\delta_{\alpha\beta}+\beta_\alpha^TC^{-1}\beta_\beta)^{-1},
\ee
where the bracketed inverse is with respect to $\alpha$, $\beta$, and
\be
\label{eq:lambdabarf}
\overline{\lambda}_\alpha = Z_{\alpha\beta}\beta_\beta^TC^{-1}(D-T),
\ee
we end up with
\be
\label{eq:integrationf}
P(T|D)\propto\int d^n\lambda\, \exp\left(-\half(\lambda_\alpha-\overline{\lambda}_\alpha) Z_{\alpha\beta}^{-1}(\lambda_\beta-\overline{\lambda}_\beta) - \half\chi^2\right) \propto \exp(-\half\chi^2).
\ee
Note that $\chi^2$ in this expression is given by Eq.~(\ref{eq:chisq}) but where $S = \beta_\alpha\beta^T_\alpha$. Note also the analogy between this and Eqn.~\ref{eq:margresult}. 

We can then use Bayes' Theorem to get the posterior distribution, 
\be
\label{eq:posteriorf}
P(\lambda|TD)\propto \exp\big(-\half(\lambda_\alpha-\overline{\lambda}_\alpha) Z_{\alpha\beta}^{-1}(\lambda_\beta-\overline{\lambda}_\beta)\big),
\ee
and from this the expectation and covariance of $\lambda_\alpha$ are
\be
\label{eq:meanvarlamf}
E[\lambda_\alpha] =\lambdabar_\alpha,\qquad E[(\lambda_\alpha-\lambdabar_\alpha)(\lambda_\beta-\lambdabar_\beta)] = Z_{\alpha\beta}.
\ee
If we write $\beta = e_\alpha \beta_\alpha$, such that $e_\alpha$ is a unit eigenvector of $Z_{\alpha \beta}$, then the corresponding eigenvalue is $z = (1+\beta^TC^{-1}\beta)^{-1})$. This means $0<z<1$ and so $Z_{\alpha\beta}$ is positive definite, therefore invertible. What's more, if all $z <1$ then $\delta_{\alpha\beta}-Z_{\alpha\beta}$ is also positive definite. We can view $Z_{\alpha\beta}$ as the ``decorrelation matrix", upgrading it from the decorrelation coefficient from Sec.~\ref{subsec:nuis} as there are now many sources of uncertainty. 

We can write, in analogy with before,
\be
\label{eq:zdefmat2}
Z_{\alpha\beta} = \delta_{\alpha\beta}-\beta_\alpha^T(C+S)^{-1}\beta_\beta,
\ee
which allows us to rewrite $\overline{\lambda}_\alpha$, using $(1-(C+S)^{-1}S)C^{-1} = (C+S)^{-1}$, as
\be
\label{eq:lambdabarfx}
\overline{\lambda}_\alpha = \beta_\alpha^T(C+S)^{-1}(D-T[f]).
\ee

\subsection{Fitting PDFs with fixed parametrisation}
Now we can use the previous section's results in the context of a PDF fit with MHOUs. In this section, we consider a fixed parametrisation of PDFs, like that adopted by, for example MSHT, CTEQ and ABM. Here the PDFs, $f(\theta)$, depend on $m$ parameters, $\theta_p$, $p=1, \dots , m$, where $m < N$ such that the data are able to determine all the parameters through $\chi^2$ minimisation. We will move on to the somewhat different case of PDFs with neural networks (unsurprisingly relevant to NNPDF) in the next section.

We adopt the same approach as in Sec.~\ref{sec:p2}, but fitting $m$ parameters, $\theta_p$, rather than a single one, $\theta$. Writing the PDF that minimises the $\chi^2$ as $f(\theta^0)\equiv f_0$, and using the notation $T_0\equiv T(\theta^0)$, $T_p\equiv \partial T(\theta^0)/\partial\theta_p^0$ with summation convention for $p$, we can linearise $T(\theta)$ as
\be
\label{eq:Tlinf}
T(\theta) = T_0 + (\theta_p-\theta_p^0)T_p + \dots .
\ee
Minimising this with respect to $\theta^p$, we find
\be
\label{eq:arep2f}
\theta^{(r)}_p - \theta^0_p = (T_p^T(C+S)^{-1}T_q)^{-1}T_q^T(C+S)^{-1}(D^{(r)}-D),
\ee
where the inverse of the left section is with respect to $p,q$. So
\bea
\Cov_{pq}[\theta] &=& \langle(\theta^{(r)}_p-\theta_p^0)(\theta^{(r)}_q-\theta_q^0)\rangle\nn\\
&=& (T_p^T(C+S)^{-1}T_q)^{-1}.
\label{eq:varaf}
\eea
To find $X$, we can separate out the magnitude and direction of $T_p$ by writing $T_p = |T_p|n_p$, where $n_p$ are (not necessarily orthogonal) unit vectors. Then Eq.~\ref{eq:Xdef3} becomes
\be
X\equiv\Cov[T[f]] = n_p(n_p^T(C+S)^{-1}n_q)^{-1}n_q^T.
\label{eq:Xdeffpq}
\ee
Note that $X(C+S)^{-1}$ still projects the data replicas onto the theory replicas, and the projective relation for $X$ still holds. 

\subsubsection{Autopredictions}
First consider the autopredictions, $T(f,\lambda)\equiv T[f]+\lambda_\alpha\beta_\alpha$. We can see that the results from Sec.~\ref{sec:p2} still hold. In particular, the central values of $\overline{\lambda}_\alpha$ are given by Eq.~\ref{eq:lambdabarfx}, and
the shifts (Eq.~\ref{eq:shift}) are now
\be
\label{eq:shiftmult}
\delta T[f] = \beta_\alpha\beta_\alpha^T(C+S)^{-1}(D^{(0)}-T[f_0]) = S(C+S)^{-1}(D{(0)}-T[f_0]).
\ee
The covariance of $\lambda$ becomes an equation for the decorrelation matrix rather than a decorrelation coefficient:
\be
\label{eq:Zbardefab}
\Cov_{\alpha\beta}[\lambda] = \delta_{\alpha\beta} -\beta_\alpha^T(C+S)^{-1}X(C+S)^{-1}\beta_\beta\equiv \Zbar_{\alpha\beta}.
\ee
Like before, both $\Zbar_{\alpha\beta}$ and $\delta_{\alpha\beta}-\Zbar_{\alpha\beta}$ are positive semidefinite, so all eigenvalues of $\Zbar_{\alpha\beta}$ lie between zero and one. 

The covariance of the autopredictions then becomes
\be
{\Cov}[T(f,\lambda)] = X+\beta_\alpha\Zbar_{\alpha\beta}\beta_\beta^T = X + S - S(C+S)^{-1}X(C+S)^{-1}S.\label{eq:covTfitf}
\ee

\subsubsection{Predictions for new observables}
Now consider true predictions for new observables. The shifts (Eq.~\ref{eq:shiftpred}) can be written
\be
\label{eq:shiftpredf}
\delta \Ttil[f] = \betatil_\alpha\beta_\alpha^T(C+S)^{-1}(D^{(0)}-T[f_0]) = \Shat (C+S)^{-1}(D^{(0)}-T[f_0]),
\ee
where we have defined $\Shat = \betatil_\alpha\beta_\alpha^T$. 

If the predictions are fully correlated, then $\Ttil(f,\lambda)=\Ttil[f]+\lambda_\alpha\betatil_\alpha$ and
\be
{\Cov}[\Ttil(f,\lambda)]
= \Xtil + \betatil_\alpha\Zbar_{\alpha\beta}\betatil_\beta^T = \Xtil + \Stil - \Shat(C+S)^{-1}X(C+S)^{-1}\Shat\label{eq:covTtilfitf},
\ee
where $\Stil = \betatil_\alpha\betatil_\alpha^T$. Finally, we have 
\be
\Xtil\equiv\Cov[\Ttil[f]] = \Ttil_p(T_p^T(C+S)^{-1}T_q)^{-1}\Ttil_q^T.
\label{eq:Xtildeff}
\ee

\subsection{Fitting NNPDFs}
In NNPDF we don't use a fixed parametrisation, but instead have a neural network with a very large number of parameters, in general greater than the number of data points. Here the ability to overfit is a danger, so we adopt a cross-validation procedure when finding the optimal $\chi^2$ (see Chapter \ref{chapter:background}). This means that when fitting each data replica, $D^{(r)}$, the $\chi^2$ is not exactly minimised; there is random noise in the system which amounts to a ``functional uncertainty"~\cite{Ball:2014uwa}. We will see in this section that this is responsible for a third mechanism of decorrelation, on top of those due to the experimental uncertainties and the loss of information in the fitting process.

The fact that Eqn.~\ref{eq:chi2rep} is not exactly minimised makes the analytical approach imprecise, and while in general the results in Sec.~\ref{subsec:p31} are valid, the exact relations for the fitted parameters (e.g. Eqn.~\ref{eq:arep2f}) and subsequent results do not hold. We are still able to use the PDF replicas compute, for example
\be
X\equiv\Cov[T[f]] = \langle (T^{(r)} - T^{(0)}) (T^{(r)} - T^{(0)})^T\rangle,
\label{eq:Xdefgen}
\ee
where $T^{(r)}\equiv T[f^{(r)}]$, and $T^{(0)}\equiv \langle T^{(r)}\rangle$. However, the projective relation for $X$ is no longer satisfied, and now $X(C+S)^{-1}$ doesn't project the data replicas on to the theory replicas.

The data and theory replicas no longer have an analytic connection, so we have to look instead at their correlations, which we can explicitly compute as 
\be
Y\equiv\Cov[T[f],D] = \langle (T^{(r)} - T^{(0)}) (D^{(r)} - D^{(0)})^T\rangle.
\label{eq:Ydefgen}
\ee
Note that $Y$ is in general not symmetric. In the case of a fixed parametrisation, such as in the previous section, then using Eq.~\ref{eq:projection} and Eq.~\ref{eq:repavD} we find $Y=Y^T=X$.

We will now redetermine the covariance of the nuisance parameters, then the autopredictions, and finally the external predictions. This time results will be in terms of $C$, $S$, $X$ and $Y$.

\subsubsection{Covariance of nuisance parameters}
Starting with
\be
\label{eq:lambdabarfxrep}
\overline{\lambda}^{(r)}_\alpha = \beta_\alpha^T(C+S)^{-1}(D^{(r)}-T^{(r)}),
\ee
we can compute
\begin{eqnarray}
\langle(\lambdabar_\alpha^{(r)}-\lambdabar_\alpha^{(0)})(\lambdabar_\beta^{(r)}-\lambdabar_\beta^{(0)})\rangle
&=& \beta_\alpha^T (C+S)^{-1}(C+S+X-Y-Y^T) \qquad \\ && \qquad \times (C+S)^{-1} \beta_\beta,\nn\\
&=& \beta_\alpha^T (C+S)^{-1}\beta_\beta + \beta_\alpha^T(X-Y-Y^T) \qquad \\ && \qquad \times  (C+S)^{-1} \beta_\beta,\label{eq:covlambdagen}
\end{eqnarray}
so
\be
\label{eq:Zbardefgen}
\Cov_{\alpha\beta}[\lambda] = \delta_{\alpha\beta} +\beta_\alpha^T(C+S)^{-1}(X-Y-Y^T)(C+S)^{-1}\beta_\beta\equiv \Zbar_{\alpha\beta}.
\ee
Note that when $X=Y=Y^T$ we get back Eqn.~\ref{eq:Zbardefab}. Also, although $\Zbar_{\alpha\beta}$ is still positive semidefinite, the eigenvalues are not bounded above by one any more, as $Y + Y^T$ is not generally positive definite. This means the presence of functional uncertainty can add another decorrelation effect which can be large enough to actually increase the theoretical uncertainties!

\subsubsection{Covariance of autopredictions}
The shifts in the autopredictions are still given by Eqn.~\ref{eq:shiftmult}, but the covaraince of autopredictions isn't the same. First note that
\bea
\label{eq:Talgebra}
T^{(r)} + \overline{\lambda}_\alpha^{(r)}\beta_\alpha &=& T^{(r)} + S(C+S)^{-1}(D^{(r)}-T^{(r)}) \\ &=&  C(C+S)^{-1}T^{(r)}  + S(C+S)^{-1}D^{(r)},
\eea
so
\begin{eqnarray}
\label{eq:covTbarx}
\Cov[T + \overline{\lambda}_\alpha\beta_\alpha] &=& C(C+S)^{-1}X(C+S)^{-1}C+S(C+S)^{-1}S \nn\\
&&\qquad+ C(C+S)^{-1}Y(C+S)^{-1}S \nn\\
&&\qquad+ S(C+S)^{-1}Y^T(C+S)^{-1}C. \nn\\
\end{eqnarray}
Then we can find the covariance matrix of predictions
\begin{eqnarray}
P\equiv\Cov[T(f,\lambda)] &=&\beta_\alpha Z_{\alpha\beta}\beta_\beta^T + \Cov[T + \overline{\lambda}_\alpha\beta_\alpha]\nn\\
&=& S+C(C+S)^{-1}X(C+S)^{-1}C \nn\\
&&\qquad+ C(C+S)^{-1}Y(C+S)^{-1}S \nn\\
&&\qquad+ S(C+S)^{-1}Y^T(C+S)^{-1}C \label{eq:covTfitfun}\\
&=&X +  \beta_\alpha \Zbar_{\alpha\beta}\beta_\beta^T - (X-Y)(C+S)^{-1}S \nn\\
&&\qquad- S(C+S)^{-1}(X-Y^T) \label{eq:covTfitfun2}.
\end{eqnarray}
Note again that when the functional uncertainty is zero, $X=Y=Y^T$ and we get back Eqn.~\ref{eq:covTfitf}. 

Now let's think about the case when the functional uncertainty is large. Then the theory replicas are almost totally uncorrelated with the data replicas, so $Y$ is much smaller than $C+S$ and $X$. In that situation 
\be
P \sim P_{\rm sim} =  S+C(C+S)^{-1}X(C+S)^{-1}C\label{eq:covTfitfunapprox},
\ee 
which is a little below the conservative result, $P_{\rm con}=X+S$. 

Revisiting the model (Eqn.~\ref{eq:modelCS}) a final time, we can look explicitly at the relative sizes of these. Using $X= x^2 nn^T$ for a unit vector, $n$, satisfying $e^Tn\equiv\cos\phi$. Then the ratio to the conservative result is
\bea
\Rbar_{\rm sim} &=& \frac{e^T(S+C(C+S)^{-1}X(C+S)^{-1}C)e}{e^T(S+X)e}\nn\\
&=& 1 - \frac{s^2(s^2+2\sigma^2)x^2\cos^2\phi}{(s^2+\sigma^2)^2(s^2+x^2\cos^2\phi)}\nn\\
&\sim&  \begin{cases}{1 -\smallfrac{ 2s^2}{\sigma^2} \qquad\qquad\mbox{for $s^2 \ll \sigma^2$ or $x^2$;}}\\{1-\smallfrac{x^2\cos^2\phi}{s^2}\qquad \mbox{for $s^2 \gg \sigma^2$ or $x^2$.}}\end{cases} 
\label{eq:modelRbarapprox}
\eea
Fig.~\ref{fig:plot3} shows a plot of this ratio. If $\sigma$ and $x$ are of similar size then $\Rbar_{\rm sim}$ always remains close to one, and there is almost total decorrelation, regardless of whether $s$ is small or large.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[h]
  \begin{center}
      \includegraphics[width=0.5\textwidth]{correlations/plots/plot3.pdf}
    \caption{ For neural network fit within the model Eqn.~\ref{eq:modelCS}.  Comparing the ratios to the conervative prescription of the simplified decorrelated ($\Rbar_{\rm sim}$, Eqn.~\ref{eq:modelRbarapprox}) and correlated ($\Rbar_c$, Eqn.~\ref{eq:modelRbarc}) autoprediction covariance matrices.  \label{fig:plot3}}
  \end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The effect when $Y \neq 0$ is also sketched. We understand that this will be modest, since the difference between $P_{\rm sim}$ and $P$ is small for both small and large $S$. However, we can get a slight over-decorrelation such that the true result actually goes slightly above the uncorrelated (conservative) result. What's more, this is in a region of relative uncertainties which is occupied by many datasets in current NNPDF fits. 

\subsubsection{Covariance of predictions for new observables}
Finally, consider predictions for new observables. The shifts are given by Eqn.~\ref{eq:shiftpredf}, and the covariance is
\begin{eqnarray}
\Ptil\equiv{\Cov}[\Ttil(f,\lambda)]
&=& \betatil_\alpha Z_{\alpha\beta}\betatil_\beta^T + \Cov[\Ttil + \overline{\lambda}_\alpha\betatil_\alpha]\nn\\
&=& \Xtil + \Stil + \Shat(C+S)^{-1}(X-Y-Y^T)(C+S)^{-1}\Shat^T  \nn\\
&&\qquad -(\Xhat-\Yhat)(C+S)^{-1}\Shat^T \nn\\
&&\qquad -\Shat(C+S)^{-1}(\Xhat^T-\Yhat^T) \label{eq:covTtilfitfun}\\
&=&\Xtil +  \betatil_\alpha \Zbar_{\alpha\beta}\betatil_\beta^T - (\Xhat-\Yhat)(C+S)^{-1}\Shat^T \nn\\
&&\qquad - \Shat(C+S)^{-1}(\Xhat^T-\Yhat^T) \label{eq:covTtilfitfun2}.
\end{eqnarray}
Note that we have defined the additional relations:
\begin{eqnarray} 
\Xtil&\equiv&\Cov[\Ttil[f]] = \langle (\Ttil^{(r)} -\Ttil^{(0)}) (\Ttil^{(r)} - \Ttil^{(0)})^T\rangle\label{eq:Xtildefgen},\\
\Xhat&\equiv&\Cov[\Ttil[f],T[f]] = \langle (\Ttil^{(r)} -\Ttil^{(0)}) (T^{(r)} - T^{(0)})^T\rangle\label{eq:Xhatdefgen},\\ 
\Yhat&\equiv&\Cov[\Ttil[f],D] = \langle (\Ttil^{(r)} -\Ttil^{(0)}) (D^{(r)} - D^{(0)})^T\rangle.
\end{eqnarray}
Again, we get the fixed parametrisation result when both $Y=Y^T=X$ and $\Yhat = \Xhat$. When there is a large functional uncertainty, $Y$ and $\Yhat$ are both small, and so
\bea
\Ptil_{\rm sim} &=&  \Xtil+\Stil + \Shat(C+S)^{-1}X(C+S)^{-1}\Shat^T \nn\\
&&\qquad -  \Xhat(C+S)^{-1}\Shat^T-\Shat(C+S)^{-1}\Xhat^T, \label{eq:covTtilfitfunapprox}
\eea 
which we expect to be close to the conservative predictions, $\Ptil_{\rm con}\equiv\Xtil+\Stil$.

\section{Numerical results}
\label{sec:p4}
Still with us? In this section we will apply all the results we worked up to in Sec.~\ref{sec:p3}. 



\section{Summary}
\label{sec:p5}
