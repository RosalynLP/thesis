\chapter{Making predictions using PDFs with theoretical uncertainties}

Earlier in this thesis we have discussed the importance of theoretical uncertainties, and how to include them in PDFs. We have also produced PDFs including MHOUs (see Chapter~\ref{chapter:mhous}) and deuteron and nuclear uncertainties (see Chapter~\ref{chapter:nuclear}), which are freely available. In the future, PDFs with theory uncertainties will become the norm, and will be used widely to make theoretical predictions for observables by convoluting them with parton-level hard cross sections. 

When making predictions for hadronic observables there are two sources of uncertainty: the hard cross section and the PDF. Typically, MHOUs for the former are estimated using scale variation, and these are added in quadrature to the latter. When the PDFs themselves also include MHOUs, we can think of the two scale evolutions considered being $Q_0 \to Q_{dat}$ and $Q_0 \to Q_{pred}$, where $Q_0$ is the PDF parametrisation scale, $Q_{dat}$ is the scale of data in the PDF and $Q_{pred}$ is the scale the prediction is made at. Then each of these sources has both a contribution from renormalisation scale variation and from factorisation scale variation.  The PDFs themselves contain a wealth of data from various processes, so when making a prediction for a process which is included in the PDFs there will invariably be correlations between the renormalisation scale variation in the PDF and that in the hard cross section. Even if the process is a new one, for example Higgs production, correlations due to factorisation scale variation will always be present. Simply combining the two sources of uncertainty in quadrature will miss these correlations and lead to an inflation in overall uncertainties. 

This issue was explored in detail in \cite{Harland-Lang:2018bxd}, noting that PDFs are a tool to express one observable in terms of others. This can be realised exactly in the simple case of fully correlated factorisation scale variation for non-singlet structure functions. Here the PDF can be eliminated entirely, and it is manifest that there is only one independent scale ($Q_{dat} \to Q_{pred}$), rather than two, with the MHOUs cancelling to a large degree. If MHOUs were used in both the PDF and the hard cross section in this case, it would amount to ``double counting". It was also shown that correlations existed for renormalisation scale variation, albeit less strongly.

When using PDFs in predictions we include both sources of scale variation in an uncorrelated way, and so miss the MHOU cancellation and corresponding reduction in uncertainties. As noted in \cite{MHOUbig}, this is a consequence of PDFs being universal. We cannot reconstruct the full data and MHOUs from the PDFs alone as information is lost in the fitting process; one set of PDFs could arise from many different data possibilities. However, if we want PDFs to be useful in a wide range of predictions, universality is required and so we must live with this loss of correlation.

Fortunately, the increase in PDF uncertainties due to MHOUs is small, and the effect is mostly realised in changes to the central value as the fit is rebalanced by changes to the weighting of different data. In indicative cases explored in Chapter 7 of \cite{MHOUbig}, it was seen that the PDF uncertainty is consequently much smaller than the MHOU on the hard cross section. When combining these two in quadrature, the effect of missing correlations will therefore likely be small, and so the overestimate of uncertainty will be small. It was argued that a small overestimate of uncertainty is better than neglecting MHOUs altogether. However, accounting for these correlations is desirable in order to keep uncertainties as low as possible in the current high precision era.

In this chapter we investigate correlations between PDF MHOUs and hard cross section MHOUs when making predictions. Although we focus on MHOUs, the analysis extends naturally to all sources of theory uncertainty. We develop a method for algebraically determining these correlations, and show how to include them when making a prediction. This is a complicated problem so we proceed incrementally:

\begin{itemize}
\item In Sec.~\ref{sec:p1} we show how a theoretical uncertainty can be reformulated in terms of a nuisance parameter, which holds the key to the propagation of uncertainties. We consider as a starting point an exact fit, showing that the experimental data partially decorrelate the theory uncertainties in the fit and those in the prediction.
\item In Sec.~\ref{sec:p2} we move on to a model where the data are fitted using just one parameter. Here the correlations lead to a shift in the theoretical predictions, but the fitting of the data leads to loss of information and consequently further decorrelation.
\item In Sec.~\ref{sec:p3} we extend this analysis to a multi-parameter fit with multiple theory uncertainties, and then to a PDF fit, where the PDFs are continuous functions with a functional uncertainty. We show that this functional uncertainty leads to a further decorrelation in the MHOUs. 
\item In Sec.~\ref{sec:p4} we present numerical results comparing this procedure with the naive approach of adding the uncorrelated contributions in quadrature, in the context of the NLO global fit with MHOUs discussed in Chapter \ref{chapter:mhous}. We make predictions including MHOUs for repetitions of all the experiments already in the fit (so-called ``autopredictions"). We then investigate the scenario of a prediction for a process already in the fit (top production), and for a new process (Higgs production), and show that including these correlations leads to a shift in the central value of the prediction that is well within the total uncertainty, and that the decorrelation of the MHOUs is almost complete. We see that in practice adding the two contributions in quadrature and keeping them uncorrelated (the ``conservative" prescription of Chapter \ref{chapter:mhous}) is remarkably accurate. Having said that, the methodology for including full correlations is now available. Finally, we suggest a novel way of improving theoretical predictions, using ths correlated shift.
\item In Sec.~\ref{sec:p5} we provide a summary.
\end{itemize}

\section{Predictions with correlated theory uncertainties}
\label{sec:p1}

We saw in Chapter \ref{chapter:thuncs} that to include theory uncertainties in a fit all you need to do is add a theory covariance matrix, $S_{ij}$, to the experimental covariance matrix $C_{ij}$. Recall that $i,j = 1, \dots, N_{dat}$ run over data points. The only assumptions underlying this result are that all uncertainties are Gaussian, and that the theory uncertainties are independent of the experimental data. Since Gaussian experimental uncertainties are already assumed in NNPDF's framework, these assumptions are very reasonable. We can express the result as the conditional probability
\be
\label{eqn:ptd}
P(T|D) \propto \exp \bigg( -\frac{1}{2} (T-D)^T(C+S)^{-1}(T-D) \bigg).
\ee
Recall that both $C$ and $S$ are real and symmetric, that $C$ is positive definite and that $S$ is positive semidefinite and will generally possess many zero eigenvalues. In a fit we determine $T$ from $D$ by maximising $P(T|D)$, which amounts to minimising
\be
\label{eqn:chi2}
\chi^2 =  (T-D)^T(C+S)^{-1}(T-D)
\ee
with respect to the free parameters which characterise the theory prediction.

In this section we start off by considering one single source of fully correlated theory uncertainty, so that
\be 
S = \beta \beta^T,
\ee
where $\beta$ are real and non-zero. 

\subsection{Nuisance parameters}
We can model the theory uncertainty as a fully correlated shift in the theory prediction:
\be 
T \to T + \lambda \beta,
\ee 
where $\lambda$ is a nuisance parameter characterising the scale of the shift. We will now show that this will lead us to Eqn.~\ref{eqn:ptd}. Firstly, assuming Gaussian experimental uncertainties, we can write
\be 
\label{eqn:nuisptd}
P(T|D\lambda) \propto \exp \bigg( -\frac{1}{2} (T + \lambda \beta -D)^T C^{-1}  (T + \lambda \beta -D) \bigg).
\ee
Using Bayes' Theorem, 
\be 
\label{eqn:bayes}
P(T|D\lambda) P(\lambda |D) = P(\lambda | TD)P(T|D).
\ee
We want to find $P(T|D)$, so we need an expression for the prior for $\lambda$, $P(\lambda |D) = P(\lambda)$, where we assume that the theory uncertainty is independent of the experimental data. We choose a unit-width Gaussian centred on zero, 
\be 
\label{eqn:lambdaprior}
P(\lambda) \propto \exp \bigg(-\frac{1}{2} \lambda^2 \bigg).
\ee
Marginalising over $\lambda$, Eqn.~\ref{eqn:nuisptd} becomes
\be 
\label{eqn:ptd2}
P(T|D) \propto \int d\lambda \exp \bigg( -\frac{1}{2} \bigg[ (T + \lambda \beta -D)^T C^{-1} (T + \lambda \beta -D) + \lambda^2 \bigg] \bigg).
\ee
We can evaluate the term in $[ \cdot ]$ by remembering $S= \beta \beta^T$, introducing the variable
\be 
\label{eqn:z}
Z \equiv (1 + \beta^T C^{-1} \beta)^{-1},
\ee
and completing the square:
\be 
\begin{split}
[ \cdot ] &= (T-D)^T C^{-1}(T-D) + (T-D)^T C^{-1} \lambda \beta + \lambda \beta^T C^{-1} (T-D) \\ &+ \lambda \beta^T C^{-1} \lambda \beta + \lambda^2 \\
& = (T-D)^T C^{-1}(T-D) + (T-D)^T C^{-1} \lambda \beta + \lambda \beta^T C^{-1} (T-D) + \lambda^2 Z^{-1} \\
&=  Z^{-1}(\lambda + Z \beta^T C^{-1} (T-D))^2 \\ &- Z(\beta^T C^{-1} (T-D))^2 + (T-D)^T C^{-1}(T-D) \\
&= Z^{-1}(\lambda + Z \beta^T C^{-1} (T-D))^2 \\ &+ (T-D)^T (C^{-1} - Z C^{-1} S C^{-1})(T-D).
\end{split}
\ee
Finally, we can use the Sherman-Morrison formula and Eqn.~\ref{eqn:chi2} to write this as
\be 
\begin{split}
[ \cdot ] &= Z^{-1}(\lambda + Z \beta^T C^{-1} (T-D))^2 + (T-D)^T (C + S)^{-1}(T-D) \\
&= Z^{-1}(\lambda + Z \beta^T C^{-1} (T-D))^2 + \chi^2 \\
&\equiv Z^{-1}(\lambda - \bar{\lambda}) + \chi^2,
\end{split}
\ee
where we have defined 
\be 
\label{eqn:lambdabardef}
\bar{\lambda} = Z \beta^T C^{-1} (D-T). 
\ee

Plugging this back into Eqn.~\ref{eqn:ptd2}, we get
\be  
\begin{split}
P(T|D) &\propto \int d\lambda \ e^{-\frac{1}{2} \chi^2} \exp \bigg( -\frac{1}{2} Z^{-1} (\lambda - \bar{\lambda})^2 \bigg) \\
&\propto e^{-\frac{1}{2} \chi^2},
\end{split}
\ee
which is Eqn.~\ref{eqn:ptd}. The advantage of this approach is that we can also get the posterior distribution for $\lambda$ (after fitting using $D$ and $T$), by using Bayes' Theorem (Eqn.~\ref{eqn:bayes}):
\be 
\begin{split}
\label{eqn:lambdaposterior}
P(\lambda |TD) &= \frac{P(T|D\lambda) P(\lambda)}{P(T|D)} \\
&= \exp \bigg( -\frac{1}{2} \bigg[ (T+ \lambda \beta -D)^T C^{-1}(T+ \lambda \beta -D) + \lambda^2 - \chi^2 \bigg] \bigg) \\
& = \exp \bigg( -\frac{1}{2} Z^{-1}(\lambda - \bar{\lambda})\bigg),
\end{split}
\ee
where we recognised the similarity between the exponent here and in Eqn.~\ref{eqn:ptd2}. So the effect of the fit is to shift the centre of the distribution from 0 $\to \bar{\lambda}$, and the width from 1 $\to Z$. Note that from the definition of $Z$ (Eqn.~\ref{eqn:z}), 
\be 
0 \le Z \le 1,
\ee
so the theory uncertainty is always reduced when information on $D$ is added. Fig.~\ref{fig:lambdadistribs} gives a sketch of this effect.  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[H]
  \begin{center}
      \includegraphics[width=0.5\textwidth]{correlations/plots/lambdapriorpost.png}
    \caption{Sketch of the prior (Eqn.~\ref{eqn:lambdaprior}) and posterior (Eqn.~\ref{eqn:lambdaposterior}) distributions for $\lambda$. Adding information shifts the distribution and reduces the width. \label{fig:lambdadistribs}}
    
  \end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Decorrelation}
We will now test out this formalism in the case of a ``perfect fit". This is one where the theory inputs to the fit are flexible enough to fit the data exactly. It is not a especially realistic situation in the context of PDF fits, but will serve as a useful foundation on which to model more complex situations. 

In this perfect fit $P(T|D)$ (Eqn.~\ref{eqn:ptd2}) is maximised when $T=D$, which corresponds to $\chi^2 = 0$. The data and theory are therefore fully correlated, and the expectation value, $E[\cdot]$, and covariance, $\Cov[\cdot]$, of the theory are
\be
\label{eq:perfectfit}
E[T] = D,\qquad \Cov[T] = \Cov[D] = C+S.
\ee
Looking back at the definition of $\bar{\lambda}$, we can see that after the fit $\bar{\lambda}=0$, and so Eqn.~\ref{eqn:lambdaposterior} is independent of $T$. The posterior for $\lambda$ has 
\be 
\label{eq:meanvarlam}
E[\lambda] =0,\qquad \Var[\lambda] = Z.
\ee
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[H]
  \begin{center}
      \includegraphics[width=0.5\textwidth]{correlations/plots/lambdaperfectfit.png}
    \caption{Sketch of the prior (Eqn.~\ref{eqn:lambdaprior}) and posterior (Eqn.~\ref{eqn:lambdaposterior}) distributions for $\lambda$ in the case of a perfect fit. The additional information from $D$ reduces the uncertainties but there is no shift as the theory fit the data exactly. \label{fig:lambdaperfect}}
   
  \end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Fig.~\ref{fig:lambdaperfect} shows a sketch of the effect on the distribution of $\lambda$. What happens if we fit $T$ to $D$ and then use the results of this fit to make predictions for $T$? We can call this an ``autoprediction", because we are making predictions about the theory included in the fit itself. In effect this is like making theory predictions for another set of experimental results where the settings tie up exactly with those already included in the fit. In this scenario, the uncertainty is reduced as the experimental data have added information. Remembering that we can write $T(\lambda) = T + \lambda \beta$, we find
\be
\label{eq:perfectcovT}
\begin{split}
&E[T(\lambda)] = E[T]=D \\ &{\Cov}[T(\lambda)] 
= {\Cov}[T] + \Var[\lambda] \beta\beta^T = C+S+ZS.
\end{split}
\ee
There are three contributions to the covariance: the experimental uncertainty in the data, the theory uncertainty in the fit, and the theory uncertainty in the prediction. Note that the theory uncertainty in the prediction is reduced by a factor $Z={\rm Var}[\lambda]$, because of the correlation between this and the theory uncertainty in the fit. Note that:
\begin{itemize}
\item ignoring correlations amounts to setting $Z=1$ and we get ${\Cov}[T(\lambda)] = C + 2S$. This is the ``conservative prescription" of Chapter.~\ref{chapter:mhous}.
\item adopting the approach of \cite{Harland-Lang:2018bxd} and assuming full correlations amounts to setting $Z=0$ and we get ${\Cov}[T(\lambda)] = C + S$, so the conservative prescription double counts with respect to this.
\end{itemize}
In reality, the situation is somewhere between these two; we see a partial decorrelation due to the presence of experimental uncertainties.

To get a better sense of the size of decorrelation, let's consider a simple model where the experimental covariance matrix is diagonal and the theory uncertainty is fully correlated. Writing $\beta = s e$, where $s$ is the size of correlated theory uncertainty and $e^Te=1$, we have
\be
\label{eq:modelCS}
C = \sigma^2 \mathbb{I},\qquad S = s^2 e e^T,
\ee
where $\sigma$ is the per-point uncertainty. Then we can evaluate:
\be
\begin{split}
\label{eq:modelCplusSinv}
(C+S)^{-1} &= \frac{1}{\sigma^2}\left(1-\frac{s^2}{\sigma^2+s^2}e e^T\right); \\
Z &= (1+s^2/\sigma^2)^{-1}.
\end{split}
\ee
so we see that the extent of decorrelation depends on the ratio $s^2/\sigma^2$:
\begin{itemize}
\item as $\sigma^2/s^2 \to 0$ we tend to the case of no experimental uncertainties, as considered in \cite{Harland-Lang:2018bxd}, and $Z \to 0$.
\item as $s^2/\sigma^2 \to 0$, $Z \to 1$, which is the conservative prescription.
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[h]
  \begin{center}
      \includegraphics[width=0.5\textwidth]{correlations/plots/plot1.pdf}
    \caption{ For a perfect fit within the model Eqn.~\ref{eq:modelCS}.  Comparing the ratios to the conervative prescription of the decorrelated ($R$, Eqn.~\ref{eq:modelR}) and correlated ($R_c$, Eqn.~\ref{eq:modelRc}) autoprediction covariance matrices.  \label{fig:plot1}}
  \end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Now consider the covariance matrix of autopredictions. In the conservative case this is $C + 2S$, which in the direction of $e$ is $\sigma^2 + 2s^2$. In any direction orthogonal to $e$ it is just $\sigma^2$ because there are no theory uncertainties in these directions. We can calculate the ratio of the true covariance matrix to the conservative one within the model in Eqn.~\ref{eq:modelCS}. This ratio is
\bea
R = \frac{e^T(C+S+ZS)e}{e^T(C+2S)e} &=& \frac{\sigma^2+(1+Z)s^2}{\sigma^2+2s^2}\nn\\
&=& 1-\frac{s^4}{\sigma^4+3\sigma^2 s^2 + 2s^4}\nn\\
&\sim&  \begin{cases}{1 -\smallfrac{ s^4}{\sigma^4} \qquad\mbox{for $s^2\ll\sigma^2 $;}}\\{ \half(1+\smallfrac{3\sigma^2}{2s^2})\qquad \mbox{for $s^2 \gg \sigma^2$.}}\end{cases} 
\label{eq:modelR}
\eea
We can see that
\begin{itemize}
\item when experimental uncertainties dominate then the theory uncertainties are almost entirely decorrelated. In this case the conservative prescription is almost exact; only a slight overestimate.
\item when theory uncertainties dominate then there is very little decorrelation, and the conservative prescription double counts the theory uncertainties.
\end{itemize}
When we're between these extremes, we can look at the difference between this ratio and that for the fully correlated case, which is
\bea
R_c = \frac{e^T(C+S)e}{e^T(C+2S)e} &=& 1 -\frac{s^2}{\sigma^2+2 s^2}\nn\\
&\sim& \begin{cases}{1 - \smallfrac{s^2}{\sigma^2} \qquad\mbox{for $s^2\ll \sigma^2$;}}\\{ \half(1+\smallfrac{\sigma^2}{2s^2})\qquad \mbox{for $s^2\gg\sigma^2 $.}}\end{cases} 
\label{eq:modelRc}
\eea
The two ratios are shown in Fig.~\ref{fig:plot1}. When $s^2/\sigma^2$ is small, the effects from decorrelation are the strongest. When $s^2= \sigma^2$, as is the case for many experiments in NNPDF fits, $R=\smallfrac{5}{6}$ and $R_c = \frac{2}{3}$, so the decorrelation result is half way between the fully correlated and conservative cases.


\section{Predictions from one-parameter fits}
\label{sec:p2}
In the previous section we considered a perfect fit, where after fitting $T=D$. Now we consider the slightly more realistic scenario, where the fit is not perfect, but the theory predictions, $T(\theta)$, only depend on a single parameter, $\theta$. This is a useful model because it captures the essence of the fitting problem but it is sufficiently simple that we will be able to solve it exactly. The $\chi^2$ (Eqn.~\ref{eqn:chi2}) will be minimised for some $\theta = \theta_0$, with variance $\Var [\theta]$. Once $\theta_0$ has been determined we can then make some predictions, $\Ttil(\theta_0)$, where the tilde denotes they are predictions for theories separate from the fit inputs. These predictions will have uncertainties proportional to $\Var [\theta]$. 

We have assumed that the uncertainties are Gaussian, and so they are differentiable. This means we can linearise $T(\theta)$ about $T(\theta_0) \equiv T_0$:
\be
\label{eq:Tlin}
T(\theta) = T_0 + (\theta-\theta_0)\Tdot_0 + \dots .
\ee
We want to determine the uncertainty in the fitted $\theta$, so we need to propagate the uncertainties in the data, $D$, and the theory, $T(\theta)$, into $\theta$. We can do this using the standard NNPDF approach (see Chapter ~\ref{chapter:background}) of generating pseudodata replicas, $D^{(r)}$, which are Gaussianly distributed about the actual data, $D$, with covariance $C+S$. More explicitly, we can define the average over replicas for any function, $F$, of the replicas as
\be
\label{eq:repav}
\langle F(D^{(r)})\rangle =  \smallfrac{1}{N_{\rm rep}}\sum_{r=1}^{N_{\rm rep}}F(D^{(r)}).
\ee
Then the replicas will satisfy
\be 
\label{eq:repavD}
\langle D^{(r)}\rangle \equiv D^{(0)}, \qquad \langle (D^{(r)}-D^{(0)})(D^{(r)}-D^{(0)})^T\rangle = C+S.
\ee
In the limit of $N_{rep} \to \infty$, $D^{(0)} \to D$, and for finite replicas $D^{(0)}$ differs from $D$ by an amount which tends to zero like $N_{rep}^{-1/2}$.

The fit proceeds by fitting a parameter replica, $\theta^{(r)}$, for each pseudodata replica, $D^{(r)}$, by minimising
\be
\label{eq:chi2rep}
\chi_r^2[\theta] = (T(\theta)-D^{(r)})^T(C+S)^{-1}(T(\theta)-D^{(r)}),
\ee
with respect to $\theta$. Using Eqn.~\ref{eq:Tlin}, this leads to 
 \be
\label{eq:arep}
\theta^{(r)} - \theta_0 = \frac{\Tdot_0^T(C+S)^{-1}(D^{(r)}-T_0)}{\Tdot_0^T(C+S)^{-1}\Tdot_0}.
\ee
Now $\theta_0 = \langle \theta^{(r)} \rangle$, so using the replica averages in Eqn.~\ref{eq:repavD} we find
\be
\label{eq:consistency}
\Tdot_0^T(C+S)^{-1}(D-T_0)=0,
\ee
so we can rewrite Eqn.~\ref{eq:arep} as
\be
\label{eq:arep2}
\theta^{(r)} - \theta_0 = \frac{\Tdot_0^T(C+S)^{-1}(D^{(r)}-D)}{\Tdot_0^T(C+S)^{-1}\Tdot_0}.
\ee
Using the fact that $C$ and $S$ are symmetric, 
\bea
\Var[\theta] &=& \langle(\theta^{(r)}-\theta_0)^2\rangle\nn\\
 &=& \frac{\Tdot_0^T(C+S)^{-1}\langle(D^{(r)}-D)(D^{(r)}-D)^T\rangle (C+S)^{-1}\Tdot_0}{(\Tdot_0^T(C+S)^{-1}\Tdot_0)^2}\nn\\
&=& (\Tdot_0^T(C+S)^{-1}\Tdot_0)^{-1}.
\label{eq:vara}
\eea
Note that:
\begin{itemize}
\item data points with a large dependence on $\theta$ have large $\Tdot_0$ and contribute more.
\item directions with large uncertainty, $(C+S)$, contribute less.
\end{itemize}
Now we have the uncertainty in the fitted parameter, $\theta$, we can find the fitting uncertainty. This is the covariance of $T(\theta)$ due to the experimental and theoretical uncertainties from fitting $\theta$. We will call this covariance matrix $X$. Using the fact that $E[T] = \langle T(\theta^{(r)})\rangle = T(\theta_0) = T_0$ and writing $T^{(r)} = T(\theta^{(r)})$,
\bea
X\equiv\Cov[T(a)] &=& \langle(T^{(r)}-T_0)(T^{(r)}-T_0)^T\rangle\label{eq:Xdef}\\
&=& \Tdot_0\langle(\theta^{(r)}-\theta_0)^2\rangle\Tdot_0^T\\
&=& \Tdot_0(\Tdot_0^T(C+S)^{-1}\Tdot_0)^{-1}\Tdot_0^T\label{eq:Xdef2}\\
&=& n(n^T(C+S)^{-1}n)^{-1}n^T,
\label{eq:Xdef3}
\eea
where in the last line we define $\Tdot_0 \equiv |\Tdot_0|n$, i.e. $n$ is a unit vector in the direction of $\Tdot_0$. We can see that $X$ depends only on the direction ($n$) of $\Tdot_0$, not its magnitude. Note that $X$ is singular and also that
\be
\label{eq:XsqeqX}
X = X(C+S)^{-1}X,
\ee
which will be useful later. Using Eqn.~\ref{eq:arep2} in Eqn.~\ref{eq:Tlin}, we can see that
\be
T^{(r)}-T_0 = X(C+S)^{-1}(D^{(r)}-D^{(0)}),
\label{eq:projection}
\ee
so $X(C+S)^{-1}$ projects the data replicas onto the theory replicas.

Now let's revisit the model for covariance matrices, Eqn.~\ref{eq:modelCS}. If we define the angle between the experimental uncertainties and the $\theta$ variation by $\cos \phi = n^T e$, we find that
\bea
\label{eq:denom}
n^T(C+S)^{-1}n &=& \frac{\sigma^2+s^2\sin^2\phi}{\sigma^2(\sigma^2+s^2)},\\
n^TXn &=& \frac{\sigma^2(\sigma^2+s^2)}{(\sigma^2+s^2\sin^2\phi)}.
\eea
Note that using any vector other than $n$ here gives 0. We can see that the effects from the theory uncertainty ($s$) depend on its degree of alignment with $n$, the direction of the parameter dependence. 
\begin{itemize}
\item For complete alignment, $\phi=0$ and the variance of $T$ in this direction is $(\sigma^2 + s^2)$.
\item When they are orthogonal, $\phi= \smallfrac{\pi}{2}$ and the variance is $\sigma^2$, so the theory uncertainty doesn't factor into the fitting.
\end{itemize}

\subsection{Autopredictions in a single parameter fit}
Now let's consider making autopredictions in this single parameter fit,
\be
\label{eq:autopred}
T(\theta,\lambda)=T(\theta)+\lambda\beta .
\ee
When fitting, we:
\begin{enumerate}
\item Maximise $P(T(\theta)|D)$ replica by replica to get $\{ \theta^{(r)} \}$.
\item Maximise $P(\lambda |T(\theta) D)$ replica by replica, using the values $\{ \theta^{(r)} \}$ from 1., to get  $\{ \lambda^{(r)} \}$.
\item Average over replicas to obtain the fitted results.
\end{enumerate}
For example, from Eqn.~\ref{eqn:lambdabardef} we can write
\be
\label{eq:lambdabarz}
E[\lambda] = \langle \bar{\lambda}(\theta^{(r)}) \rangle =  \overline\lambda(\theta_0) = \beta^T(C+S)^{-1}(D^{(0)}-T_0).
\ee
Now that in general $D \neq T_0$, the nuisance parameters can have non-zero expectation values, and this will contribute to the expectation value of the autopredictions:
\be
\label{eq:ET}
E[T(\theta,\lambda)] = \langle T(\theta^{(r)},\overline\lambda(\theta^{(r)})\rangle =  
T(\theta_0,\overline\lambda(\theta_0)) = T(\theta_0)+\overline\lambda_0\beta.
\ee
So the data give information on the theory uncertainty by inducing shifts in the autopredictions:
\be
\label{eq:shift}
\delta T(\theta_0) = \beta\beta^T(C+S)^{-1}(D^{(0)}-T(\theta_0)) = S(C+S)^{-1}(D^{(0)}-T(\theta_0)).
\ee
Recall that $n^T(C+S)^{-1}(D^{(0)}-T(\theta_0)) =0$ from Eqn.~\ref{eq:consistency}, so we only get non-zero shifts when $n$ and $e$ (the data and the theory) point in different directions. When they are parallel ($\theta =0$), the theory uncertainty is just absorbed into the fit.

Now let's look at the change in the uncertainty of the autopredictions relative to the original predictions. The variance of $\lambda$ has two independent contributions in this scenario:
\begin{enumerate}
\item the width of the posterior, $P(\lambda | T(\theta) D)$;
\item the fluctuation of $\lambda (\theta)$ due to changing $\theta$ over replicas.
\end{enumerate}
So we can write
\be
\label{eq:varlamdef}
\Var[\lambda] = E[(\lambda-\overline\lambda_0)^2] = E[(\lambda-\overline\lambda(\theta^{(r)})^2]+ \langle(\overline\lambda(\theta^{(r)})-\overline\lambda_0)^2\rangle.
\ee
It's easiest to first address these two contributions separately. The first term is analogous to that in Sec.~\ref{sec:p1}, and we can use the Sherman Morrison formula again to rewrite it:
\be
\begin{split}
\label{eq:Zdef2}
E[(\lambda-\overline\lambda(\theta^{(r)})^2] = Z &= (1 + \beta^T C^{-1}  \beta)^{-1} \\ &= 1-\beta^T(C+S)^{-1}\beta,
\end{split}
\ee
As for the second term, we can use the definitions of $\bar{\lambda}$ and $\bar{\lambda}_0$, and then Eqn.~\ref{eq:projection} to write
\bea
 \overline\lambda(\theta^{(r)})-\overline\lambda_0 &=& \beta^T(C+S)^{-1}(D^{(r)}-D^{(0)}-(T^{(r)}-T_0))\nn\\
&=& \beta^T(C+S)^{-1}(D^{(r)}-D^{(0)}-(\theta^{(r)}-\theta_0)\Tdot_0)\nn\\
&=& \beta^T(C+S)^{-1}(1-X(C+S)^{-1})(D^{(r)}-D^{(0)}).\label{eq:lamalgebra}
\eea
So we can then get
\bea
 \langle(\overline\lambda(\theta^{(r)})-\overline\lambda_0)^2\rangle &=&
\beta^T(C+S)^{-1}(1-X(C+S)^{-1})\langle(D^{(r)}-D^{(0)})(D^{(r)}-D^{(0)})^T\rangle\nn\\
&&\qquad\qquad \times(1-(C+S)^{-1}X)(C+S)^{-1}\beta\nn\\.
\eea
Then using Eqn.~\ref{eq:repavD} followed by Eqn.~\ref{eq:XsqeqX}:
\bea
 \langle(\overline\lambda(\theta^{(r)})-\overline\lambda_0)^2\rangle &=&\beta^T((C+S)^{-1}-2(C+S)^{-1}X(C+S)^{-1}\nn\\
&&\qquad +(C+S)^{-1}X(C+S)^{-1}X(C+S)^{-1})\beta
\label{eq:varlambar1}\\
&=&\beta^T(C+S)^{-1}\beta-\beta^T(C+S)^{-1}X(C+S)^{-1}\beta
\label{eq:varlambar2}
\eea
So overall 
\be
\label{eq:Zbardef}
\Var[\lambda] = 1 -\beta^T(C+S)^{-1}X(C+S)^{-1}\beta\equiv \Zbar.
\ee
We can see that $\bar{Z} \leq 1$ because $(C+S)^{-1}X(C+S)^{-1}$ is positive semidefinite, and noting that $\bar{Z} = Z + \langle(\overline\lambda(\theta^{(r)})-\overline\lambda_0)^2\rangle$, it is clear that $\bar{Z} \geq Z$. When $n=e$, so that the $\theta$ variation and the theory uncertainty are aligned, we get $\bar{Z} = Z$. So in summary
\be
\label{zbarbounds}
0<Z\leq\Zbar\leq 1.
\ee
In other words, there is more decorrelation in this constrained fit than in the perfect fit of Sec.~\ref{sec:p1}, because the fit uncertainty causes additional decorrelation. This is the second mechanism for decorrelation we have uncovered, on top of the decorrelation due to experimental uncertainties observed in Sec.~\ref{sec:p1}.

Finally we can use the model for uncertainties (Eqn.~\ref{eq:modelCS}) to show that
\be
\label{eq:Zbarmod}
\Zbar = \frac{\sigma^2 + s^2 \sin^2\phi}{\sigma^2+s^2}.
\ee
So, comparing this with the expression for $Z$ (Eqn.~\ref{eq:modelCplusSinv}), $\bar{Z} = Z$ when $\phi =0$, as expected, and $\bar{Z}=1$ when $\phi = \smallfrac{\pi}{2}$; here the data have no effect on the uncertainty, so we have total decorrelation.

Let's also calculate the covariance of autopredictions in the model. This is made up of two contributions:
\begin{enumerate}
\item the variation about $\bar{\lambda}$;
\item the fluctuations of the individual replicas in the fit.
\end{enumerate}
\bea
{\Cov}[T(\theta,\lambda)] &=& E[(\lambda-\lambdabar(\theta))^2]\beta\beta^T 
 + \Cov[T(\theta,\lambdabar(\theta)]\label{eq:covTsumx}\nn\\
&=& ZS 
 + \langle(T(\theta^{(r)},\lambdabar(\theta^{(r)})) -T(\theta_0,\lambdabar_0))^2 \rangle  \label{eq:covTsum}
\eea
Let's first calculate
\bea
T(\theta^{(r)},\lambdabar(\theta^{(r)}))-T(\theta_0,\lambdabar_0) &=& T(\theta^{(r)}) - T(\theta_0) + (\bar{\lambda}(\theta^{(r)}) - \bar{\lambda_0}) \beta \\
&=& X(C+S)^{-1}(D^{(r)}-D^{(0)}) \\ &&\qquad + \beta \beta^T (C+S)^{-1}(1-X(C+S)^{-1}) \qquad \qquad \\ && \qquad \times (D^{(r)}-D^{(0)}) \\
&=& [ X(C+S)^{-1} \\ && \qquad + S(C+S)^{-1}(1-X(C+S)^{-1}) ] \qquad \\ && \qquad \times  (D^{(r)}-D^{(0)}).
\eea
Now consider right multiplying the term in $[ \cdot ]$ by $\mathbb{I} = (C+S)(C+S)^{-1}$, and it can be shown that
\be
T(\theta^{(r)},\lambdabar(\theta^{(r)}))-T(\theta_0,\lambdabar_0) = (S+C(C+S)^{-1}X)(C+S)^{-1}(D^{(r)}-D^{(0)}).\label{eq:Tarep}
\ee
So, using Eqn.~\ref{eq:XsqeqX} to simplify the quadratic in $X$, the second term is
\bea
{\Cov}[T(\theta,\lambdabar)]&=&\langle(T(\theta^{(r)},\lambdabar(\theta^{(r)}))-T(\theta_0,\lambdabar_0))(T(\theta^{(r)},\lambdabar(\theta^{(r)}))-T(\theta_0,\lambdabar_0))^T\rangle\nn\\ 
&=& (S+C(C+S)^{-1}X)(C+S)^{-1}(S+X(C+S)^{-1}C)\nn\\ \qquad
&=&  S(C+S)^{-1}S + X - S(C+S)^{-1}X(C+S)^{-1}S. \qquad \qquad \label{eq:covTrlamr}
\eea
So the total covariance of autopredictions is
\bea
{\Cov}[T(\theta,\lambda)] &=& (S - S(C+S)^{-1}S) + (S(C+S)^{-1}S +X - S(C+S)^{-1}X(C+S)^{-1}S)\nn\\
&=& X + S - S(C+S)^{-1}X(C+S)^{-1}S \\
&=& X + \bar{Z} S. \label{eq:covTfit2}
\eea
This is the same as if we had assumed that $T(\theta)$ and $\lambda$ are uncorrelated, i.e. that
\be
{\Cov}[T(\theta,\lambda)] = {\Cov}[T] + \Var[\lambda] \beta\beta^T 
= X+\Zbar S.\label{eq:covTfit}
\ee
So we see that the cross correlations between $T(\theta)$ and $\lambda$ have cancelled.

We can use the model again to calculate the ratio of this covariance to the one given by the conservative prescription:
\bea
\Rbar &=& \frac{e^T(X+\Zbar S)e}{e^T(X+S)e}\nn\\
&=& 1 - \frac{s^4\cos^2\phi}{\sigma^2+ s^2}\frac{\sigma^2+s^2\sin^2\phi}{\sigma^2(\sigma^2+s^2)\cos^2\phi+s^2(\sigma^2+s^2\sin^2\phi)}\nn\\
&\sim&  \begin{cases}{1 -\smallfrac{ s^4}{\sigma^4} \qquad\qquad\mbox{for $s^2 \ll \sigma^2$;}}\\{ \sin^2\phi +\smallfrac{\sigma^2{\rm cot}^2\phi}{s^2}\qquad \mbox{for $s^2 \gg \sigma^2$.}}\end{cases} 
\label{eq:modelRbar}
\eea
\begin{itemize}
\item When the experimental uncertainties dominate, $\sigma^2 \gg s^2$  and there is almost total decorrelation, as before. The conservative prescription is a very good approximation.
\item When the theory uncertainties dominate, $s^2 \gg \sigma^2$, and the extent of decorrelation is $ \propto \sin^2 \phi$. If $n$ and $e$ are closely aligned this is small, but if they're orthogonal then fitting the data has no effect on the theory uncertainty, so we have total decorrelation. In that case the assumption of full correlation is a big underestimate. 
\end{itemize}
The ratio to the full correlation case is
\bea
\Rbar_c &=& \frac{e^TXe}{e^T(X+S)e}\nn\\
&\sim& \begin{cases}{1 - \smallfrac{s^2}{\sigma^2\cos^2\phi} \qquad\mbox{for $s^2 \ll \sigma^2$;}}\\{ \smallfrac{\sigma^2{\rm cot}^2\phi}{s^2}\qquad \mbox{for $s^2 \gg \sigma^2$.}}\end{cases} 
\label{eq:modelRbarc}
\eea
Fig.~\ref{fig:plot2} shows a comparison between these two. When $\phi$ is small, the limit of $\smallfrac{s^2}{\sigma^2}$ is reached much more slowly because it requires $s^2 \gg \sigma^2 \cot^2 \phi$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[h]
  \begin{center}
      \includegraphics[width=0.5\textwidth]{correlations/plots/plot2.pdf}
    \caption{ For single parameter fit within the model Eqn.~\ref{eq:modelCS}.  Comparing the ratios to the conervative prescription of the decorrelated ($\Rbar$, Eqn.~\ref{eq:modelRbar}) and correlated ($\Rbar_c$, Eqn.~\ref{eq:modelRbarc}) autoprediction covariance matrices.  \label{fig:plot2}}
  \end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Correlated MHOUs in PDF fits}
\label{sec:p3}

\section{Numerical results}
\label{sec:p4}

\section{Summary}
\label{sec:p5}
