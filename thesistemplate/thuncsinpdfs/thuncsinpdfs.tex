\chapter{Theory uncertainties in PDFs - 10\%}
\bi 
\item What are theory uncertainties?
\item Why are they now important?
\item Types of unc - see below
\item Bayesian interpretation of these uncertainties - there is one "true" value e.g. for the higher order value, so need to estimate uncertainty bc will never know the size of e.g. MHOU unless you go ahead and calc - maybe ref d'Agostini paper on Bayesian interpretation
\item Will use Bayesian framework and assume Gaussianity of the expected true value of theory calc
\item Show C+S in fit - plus sign because exp and th unc are independent so combine errors in quadrature. They are also on an equal footing in terms of their effect on the PDFs
\item When many datasets/global fit, can have v strong theory correlations even across different experiments, because the underlying theory connects them

\ei
\section{Fitting PDFs including theory uncertainties}
Historically, experimental uncertainties have been the dominant source of error in PDF fits. 
In the NNPDF framework both replica generation and computation of $\chi^2$ 
are currently based entirely on these. We must now try to match the ongoing drive to 
increase experimental precision by including errors introduced at the theoretical level. This is
especially important given recent data sets such as the $Z$ boson transverse momentum
distributions~\cite{Aad:2014xaa}~\cite{Khachatryan:2015oaa}~\cite{Aad:2015auj}, which have very high experimental precision. Without
the inclusion of theoretical errors, this has led to tension with the other datasets.

In future NNPDF fits theoretical uncertainties will be included following a procedure outlined
by Ball \& Desphande \cite{Ball:2018odr}. This hinges on a result from Bayesian statistics
which applies to Gaussian errors. Namely, theory uncertainties can be included by directly 
adding a theoretical
covariance matrix to the experimental covariance matrix prior to the fitting. A brief summary of
the derivation is given below.

When determining PDFs we incorporate information from experiments in the form of $N_{dat}$ experimental data points $D_i$, $i=1,...,N_{dat}$. The associated uncertainties and their correlations are encapsulated in an experimental covariance matrix $C_{ij}$. Parts of the matrix which associate two independent experiments will be populated by zeros. However we would expect there to be correlations between data points from the same detector, for example.

Each data point is a measurement of some fundamental ``true" value, $\truev_i$, dictated by the underlying physics. In order to make use of the data in a Bayesian framework, we assume that the experimental values follow a Gaussian distribution about the unknown $\truev$. Then, assuming the same prior for $D$ and $\truev$, we can write an expression for the conditional probability of $\truev$ given the known data $D$:
\beq
\label{eqn:gaussexp}
P(\truev|D) = P(D|\truev) \propto \exp\bigg( -\frac{1}{2}(\truev_i-D_i)C_{ij}^{-1}(\truev_j - D_j)\bigg).
\eeq
However, in a PDF fit we cannot fit to the unknown true values $\truev$, and must make do with predictions based on current theory $T_i$. This is the origin of theory uncertainties in PDF fits; where our theory is incomplete, fails to describe the physics well enough, or where approximations are made, we will introduce all kinds of subtle biases into the PDF fit. The theory predictions themselves also depend on PDFs, so uncertainties already present in the PDFs are propagated through. This, in particular, leads to a high level of correlation because the PDFs are universal, and shared between all the theory predictions. 

We can take a similar approach when writing an expression for the conditional probability of the true values $\truev$ given the available theory predictions $T$, by assuming that the true values are Gaussianly distributed about the theory predictions.
\beq
\label{eqn:gausstheory}
P(\truev|T) = P(T|\truev) \propto \exp\bigg( -\frac{1}{2}(\truev_i-T_i)S_{ij}^{-1}(\truev_j - T_j)\bigg),
\eeq
where $S_{ij}$ is a ``theory covariance matrix" encapsulating the magnitude and correlation of the various theory errors. We will need to do some work to determine $S_{ij}$ for the different sources of error, and this will be outlined in detail in the following chapters. 

When we fit PDFs we aim to maximise the probability that a PDF-dependent theory is true given the experimental data available. This amounts to maximising $P(T|D)$, marginalised over the unknown true values $\truev$. To make this more useful for fitting purposes, we can relate this to $P(D|T)$ using Bayes' Theorem:
\beq
P(D|T)P(\truev |DT) = P(\truev |T)P(D|\truev T),
\eeq
where we note that the experimental data $D$ do not depend on our modelled values $T$, so $P(D|\truev T) = P(D|\truev)$. So we can integrate Bayes' Theorem over the possible values of the $N$-dimensional true values $\truev$:
\beq
\int D^N \truev P(D|T)P(\truev |DT) = \int D^N \truev P(\truev |T) P(D|\truev), 
\eeq
and, because $\int D^N \truev P(\truev |TD) = 1$ as all possible probabilities for the true values must sum to one, 
\beq
P(D|T) =  \int D^N \truev P(\truev |T) P(D|\truev). 
\eeq
We can always write the theory predictions $T$ in terms of their shifts $\D$ relative the true values $\truev$:
\beq
\D_i \equiv \truev_i - T_i.
\eeq
These shifts quantify the accuracy of the theoretical predictions, and can be thought of as nuisance parameters in the PDF fit. We can express the above integral in terms of the shifts $\D_i$, making use of the assumptions of Gaussianity in Eqns. \ref{eqn:gaussexp} and \ref{eqn:gausstheory}:
\beq
\begin{split}
\label{eq:probdatgivth}
P(D|T) &\propto \int D^N \D \exp \bigg(-\frac{1}{2}(D_i - T_i -\D_i)\\
&\times C_{ij}^{-1} (D_j -T_j -\D_j) - \frac{1}{2}\D_i S_{ij}^{-1} \D_j \bigg).
\end{split}
\eeq
To evalute the Gaussian integrals, consider the exponent: switching to a vector notation for the time being, we can expand this out and then complete the square, making use of the symmetry of $S$ and $C$:
\bdm
(D-T-\D)^T C^{-1}(D-T-\D) + \D^T S^{-1} \D  
= D^T(C^{-1} + S^{-1})\D -\D^T C^{-1} (D-T)
 - (D-T)^T C^{-1}\D + (D-T)^T C^{-1}(D-T) 
= (\D - (C^{-1} + S^{-1})^{-1} C^{-1}(D-T))^T(C^{-1}+S^{-1}) 
\times (\D - (C^{-1} + S^{-1})^{-1} C^{-1}(D-T)) 
-(D-T)^TC^{-1}(C^{-1} + S^{-1})^{-1}C^{-1}(D-T) 
+(D-T)^T C^{-1}(D-T).
\edm
Now, integrating Eqn. \ref{eq:probdatgivth} over $\D$ leads to a constant from the Gaussian integrals, which we can absorb, and only the parts of the exponent without $\D$ remain:
\bdm
P(T|D) = P(D|T) \propto \exp \bigg(-\frac{1}{2}(D-T)^T (C^{-1}-C^{-1}(C^{-1}+S^{-1})^{-1}C^{-1})(D-T) \bigg).
\edm
We can further simplify this by noting that
\bdm
(C^{-1}+S^{-1})^{-1} = (C^{-1}(C+S)S^{-1})^{-1} = S(C+S)^{-1}C,
\edm
which means we can rewrite
\bdm
C^{-1}-C^{-1}(C^{-1}+S^{-1})^{-1}C^{-1} = C^{-1} - C^{-1}S(C+S)^{-1} = (C^{-1}(C+S)-C^{-1}S)(C+S)^{-1} =(C+S)^{-1}.
\edm
Finally, with indices restored we are left with 
\bdm
P(T|D) \propto \exp \bigg(-\frac{1}{2}(D_i-T_i)(C+S)^{-1}_{ij}(D_j-T_j) \bigg).
\edm

Use the expression for conditional probability $P(X \cap Y)=P(X|Y)P(Y)$ and integrate over all possible values of the true theory $T$ (as this is an unknown):
%
\beq
\begin{split}
    \int dT \ P(T|y \cap f)P(y|f) &= \int dT \ P(y|T \cap f)P(T|f)\\
    P(y|f)                  &= \int dT \ P(y|T \cap f)P(T|f)\,.
\end{split}
\label{integ}
\eeq
%
Now assume Gaussian uncertainties for data and theory, of the form $\text{exp}(-\frac{1}{2}\chi^2)$
%
\beq
	P(y|Tf) \propto \text{exp}\bigg( -\frac{1}{2} (y - T)^T \sigma^{-1} (y - T)\bigg)
\eeq
%
\beq
	P(T|f) \propto \text{exp}\bigg( -\frac{1}{2} (T - T[f])^T s^{-1} (T - T[f])\bigg)
\eeq
%
and substitute these into Eq.~\ref{integ} to get
%
\beq
    P(y|f) \propto \int dT \text{ exp}\bigg(-\frac{1}{2}\bigg[ (y - T)^T \sigma^{-1} (y - T) + (T - T[f])^T s^{-1} (T - T[f])\bigg]\bigg)\,.
\eeq
%
Note that the difference between the full theory $T$ and the theory predictions $T[f]$ defines the total correction, $C$.  Therefore the substitution  $T= T[f] + C \Rightarrow dT = dT[f] + dC$ can be made, noting that $dT[f]=0$ because $T[f]$ is the fixed output of NNPDF analysis. The overall expression then becomes
%
\beq
    P(y|f) \propto \int dC \text{ exp}\bigg(-\frac{1}{2}\bigg[ (y - T[f] - C)^T \sigma^{-1} (y - T[f] - C) + C^T s^{-1} C \bigg]\bigg)
    \label{v2pyf}
\eeq
%
which can be evaluated by Gaussian integration over shifted variables, leading to
%
\beq
	P(y|f) \propto \text{exp}\bigg(- \frac{1}{2} (y - T[f])^T (\sigma + s)^{-1} (y - T[f])\bigg)\,.
\eeq
%
The final result is that you can treat theoretical errors in exactly the same was as you
treat experimental errors.

\section{Sources of Theoretical Uncertainties}

The next step is to estimate the theory covariance matrix, $s$. This can include a number of different theoretical uncertainties that may appear in PDF fits, such as:

\begin{enumerate}
    \item \textbf{Statistical uncertainties} such as from Monte Carlo generators. These provide diagonal entries to $s$.
    \item \textbf{Systematic uncertainties}. These are trickier, and can be estimated by varying some fit parameter $\xi$ from its value at the central prediction, $\xi_0$, and applying 
        \beq
        s_{ij} = \langle(T[f;\xi]-T[f;\xi_0])_i(T[f;\xi]-T[f;\xi_0])_j\rangle
        \label{theoryunc}
    \eeq
        where the angled brackets denote the averaging over a given range of $\xi$ according to some prescription.
\end{enumerate}
Including systematic uncertainties will pose the biggest challenge. We need to identify the
places they are being introduced and then make a suitable choice of $\xi$. Examples of
systematic uncertainties we have begun to address are:
    \bi
    \item \textbf{Missing higher order uncertainties (MHOUs)}. These are a result of calculations
      being done only up to a certain perturbative order in the expansion of $\alpha_s$.
      As discussed in more detail below, we can get a handle on these by varying the artificial
      renormalisation ($\mu_R$) and factorisation ($\mu_F$) scales introduced in the calculation.
      Here $\xi$ can be thought of as a vector $(\mu_R, \mu_F)$.
    \item \textbf{Nuclear and deuteron corrections}. Here data are taken from nuclear targets
      but the theoretical treatment does not account for this. We can re-calculate the
      observables under different nuclear models or parametrisations. In this case $\xi$
      indexes the model or parametrisation used. The use of multiple models helps to remove
      any systematic bias introduced by each individual one.
    \ei
\section{Renormalisation group invariance}
\section{Scale variation in partonic cross-sections}
\section{Scale variation in PDF evolution}
\section{Double scale variations}
\section{Multiple scale variations}